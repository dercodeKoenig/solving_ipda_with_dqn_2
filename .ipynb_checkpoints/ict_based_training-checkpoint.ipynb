{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490429f2-b90f-4a9d-8589-08fbe59c80c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_position_pen_th = 0.5 ## absolute of net position to add negative reward\n",
    "net_position_pen_th_value = 5\n",
    "net_position_max_th = 0.75 ## absolute of net position to add negative reward\n",
    "position_vol = 0.01\n",
    "res_high = 128\n",
    "maxlen = 384\n",
    "commission = 4/100000\n",
    "num_open = 5\n",
    "offset_limit = 1.00001 ## add to limit orders, limit = price * offset_limit for sell or  price / offset_limit for buy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b31a54-7b58-4d6b-83fa-54a0c5f80635",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports + price data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6b54a-6656-4a80-a7b6-2ef4fe1c8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import MetaTrader5 as mt5\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "#import math\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "lookback = 1000\n",
    "\n",
    "####backtest\n",
    "prices = 0\n",
    "\n",
    "def get_prices(_d1, _d2): # params for live program:\n",
    "    global prices\n",
    "    \n",
    "    t = list(prices[\"Time\"])\n",
    "    o = list(prices[\"Open\"])\n",
    "    h = list(prices[\"High\"])\n",
    "    l = list(prices[\"Low\"])\n",
    "    c = list(prices[\"Close\"])\n",
    "    \n",
    "    true_time = []\n",
    "    for i in t:\n",
    "        _time = i.split()[-1]\n",
    "        true_time.append(_time)\n",
    "        \n",
    "    \n",
    "    data = {\"Time\": true_time, \"Open\":o, \"High\":h, \"Low\":l, \"Close\":c, \"timestamp\": t}\n",
    "    df = pd.DataFrame(data)\n",
    "    #print(df)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae11e9-1ed8-4f4b-8eca-77d7148b568b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## cv2 drawing (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f04e30-2fcb-4efb-b128-98174dad99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_plot_candles(candles, pd_arrays):\n",
    "    w = 1300\n",
    "    h = 600\n",
    "    canvas = np.zeros((h,w,3), np.uint8) \n",
    "    l = len(candles)\n",
    "    single_candle_w = w / l * 0.95\n",
    "    max_h = 0\n",
    "    max_l = 1000000\n",
    "    for i in candles:\n",
    "        if i.h > max_h:\n",
    "            max_h = i.h\n",
    "        if i.l < max_l:\n",
    "            max_l = i.l\n",
    "    hlrange = max_h - max_l\n",
    "    def scale_p(p):\n",
    "        return (p - max_l) / hlrange * h\n",
    "    \n",
    "    for i in range(len(candles)):\n",
    "        if np.sum(candles[i].action) == 0:\n",
    "            continue\n",
    "        cv2.rectangle(canvas, (int(i*single_candle_w),int(0)), (int((i+1)*single_candle_w),int(20)), [x*255 for x in candles[i].ipda_true_day_time[0:3]], -1)\n",
    "        cv2.rectangle(canvas, (int(i*single_candle_w),int(20)), (int((i+1)*single_candle_w),int(40)), [x*255 for x in candles[i].ipda_true_day_time[3:6]], -1)\n",
    "            \n",
    "        color = (0,200,0) if candles[i].c > candles[i].o else (0,0,200)\n",
    "        cv2.rectangle(canvas, (int(i*single_candle_w),int(scale_p(candles[i].o))), (int((i+1)*single_candle_w),int(scale_p(candles[i].c))), color, -1)\n",
    "        cv2.line(canvas, (int((i+0.5)*single_candle_w),int(scale_p(candles[i].h))), (int((i+0.5)*single_candle_w),int(scale_p(candles[i].l))), color)\n",
    "        \n",
    "        sl = candles[i].swing_l_h[0]\n",
    "        sh = candles[i].swing_l_h[1]\n",
    "        \n",
    "        if sh:\n",
    "            cv2.circle(canvas, (int((i+0.5)*single_candle_w),int(scale_p(candles[i].h))), int(single_candle_w/2), (15,100,50), 3) \n",
    "        if sl:\n",
    "            cv2.circle(canvas, (int((i+0.5)*single_candle_w),int(scale_p(candles[i].l))), int(single_candle_w/2), (15,50,100), 3) \n",
    "        \n",
    "        bs_liquidity_taken = candles[i].buyside_taken\n",
    "        ss_liquidity_taken = candles[i].sellside_taken\n",
    "        \n",
    "        if ss_liquidity_taken:\n",
    "            cv2.rectangle(canvas, (int((i)*single_candle_w),int(scale_p(candles[i].o))), (int((i+1)*single_candle_w),int(scale_p(candles[i].c))), (255,255,0), 1)\n",
    "        if bs_liquidity_taken:\n",
    "            cv2.rectangle(canvas, (int((i)*single_candle_w),int(scale_p(candles[i].o))), (int((i+1)*single_candle_w),int(scale_p(candles[i].c))), (255,0,255), 1)\n",
    "               \n",
    "    pd_array_draw_width = 30\n",
    "    for i in pd_arrays:\n",
    "        if type(i) == liquidity:\n",
    "            color = (0,200,0) if i.l_type == -1 else (0,0,200)\n",
    "            cv2.line(canvas, (w-pd_array_draw_width-20, int(scale_p(i.price))), (int(w), int(scale_p(i.price))), color, 1)\n",
    "        \n",
    "        if type(i) == fair_value_gap:\n",
    "            color = (200,0,200) if i.bull_bear == -1 else (200,200,0)\n",
    "            if i.retraded == True:\n",
    "                color = (100,color[1],color[2])\n",
    "            if i.closed == True:\n",
    "                color = (50,color[1],color[2])\n",
    "            cv2.rectangle(canvas, (int(w-pd_array_draw_width*2),int(scale_p(i.low))), (int(w-pd_array_draw_width),int(scale_p(i.high))), color, -1)\n",
    "            \n",
    "        if type(i) == xclosedcandles_broken:\n",
    "            color = (200,0,200) if i.bull_bear == -1 else (200,200,0)\n",
    "            if i.retraded == True:\n",
    "                color = (100,color[1],color[2])\n",
    "            if i.closed == True:\n",
    "                color = (50,color[1],color[2])\n",
    "            cv2.rectangle(canvas, (int(w-pd_array_draw_width),int(scale_p(i.low))), (int(w),int(scale_p(i.high))), color, -1)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    #cv2.line(canvas, (int(d_range[0][1] * single_candle_w), int(scale_p(d_range[0][0]))), (int(d_range[1][1] * single_candle_w), int(scale_p(d_range[1][0]))), (255,0,0), 2)\n",
    "    #cv2.line(canvas, (int(p_d_range[0][1] * single_candle_w), int(scale_p(p_d_range[0][0]))), (int(p_d_range[1][1] * single_candle_w), int(scale_p(p_d_range[1][0]))), (255,50,50), 2)\n",
    "    \n",
    "    \n",
    "    canvas = canvas[::-1]\n",
    "    cv2.imshow(\"\", canvas)\n",
    "    cv2.waitKey(1)\n",
    "    #plt.figure(figsize=(20,10))\n",
    "    #plt.imshow(canvas)\n",
    "    #plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0e60b-e2cc-4ebc-b38d-c1575b346d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ta section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687a4d3e-e893-4c52-b7ce-275257f96376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### actions\n",
    "num_actions = 12\n",
    "action_swing_high_created = 0\n",
    "action_swing_low_created = 1\n",
    "action_buyside_taken = 2 \n",
    "action_sellside_taken = 3    \n",
    "\n",
    "action_fvg_bull_created = 4\n",
    "action_fvg_bear_created = 5\n",
    "action_bull_fvg_retraded = 6\n",
    "action_bear_fvg_retraded = 7\n",
    "action_bull_fvg_closed = 8\n",
    "action_bear_fvg_closed = 9\n",
    "\n",
    "action_ob_bull_created = 10\n",
    "action_ob_bear_created = 11\n",
    "\n",
    "\n",
    "### ipda true day times\n",
    "time_asian_range = [1,0,0,0,0,0] # 20:00 - 00:00\n",
    "time_midnight = [0,1,0,0,0,0] # 00:00 - 02:00\n",
    "time_london_open = [0,0,1,0,0,0] # 02:00 - 05:00\n",
    "time_london_lunch = [0,0,0,1,0,0] # 05:00 - 07:00\n",
    "time_ny_open = [0,0,0,0,1,0] # 07:00 - 10:00\n",
    "time_london_close = [0,0,0,0,0,1] # 10:00 - 12:00\n",
    "time_outside = [0,0,0,0,0,0] # 12:00 - 20:00\n",
    "\n",
    "class liquidity:\n",
    "    price = 0\n",
    "    l_type = 0 ## 1 = buyside, -1 = sellside\n",
    "    def __init__(self, price, buyside_sellside):\n",
    "        self.price = price\n",
    "        self.l_type = buyside_sellside\n",
    "        \n",
    "class fair_value_gap:\n",
    "    bull_bear = 0\n",
    "    low = 0\n",
    "    high = 0\n",
    "    retraded = False\n",
    "    closed = False\n",
    "    \n",
    "    def __init__(self, bull_bear, low, high):\n",
    "        self.bull_bear = bull_bear\n",
    "        self.low = low\n",
    "        self.closed = False\n",
    "        self.retraded = False\n",
    "        self.high = high\n",
    "\n",
    "class xclosedcandles_broken:\n",
    "    bull_bear = 0 # downclose broken = bull, upclose broken = bear\n",
    "    low = 0\n",
    "    high = 0\n",
    "    retraded = 0\n",
    "    closed = 0\n",
    "    \n",
    "    def __init__(self, bull_bear, low, high):\n",
    "        self.bull_bear = bull_bear\n",
    "        self.low = low\n",
    "        self.closed = False\n",
    "        self.retraded = False\n",
    "        self.high = high\n",
    "        \n",
    "        \n",
    "class candle:\n",
    "    def __init__(self, candle):\n",
    "        self.o = candle[1]\n",
    "        self.h = candle[2]\n",
    "        self.l = candle[3]\n",
    "        self.c = candle[4]\n",
    "        self.time = candle[0]\n",
    "        \n",
    "        hour = int(self.time.split(\":\")[0])\n",
    "        \n",
    "        \n",
    "        self.action =  [0 for _ in range(num_actions)]\n",
    "        \n",
    "        if hour >= 0 and hour < 2:\n",
    "            self.ipda_true_day_time = time_midnight\n",
    "        if hour >= 2 and hour < 5:\n",
    "            self.ipda_true_day_time = time_london_open    \n",
    "        if hour >= 5 and hour < 7:\n",
    "            self.ipda_true_day_time = time_london_lunch\n",
    "        if hour >= 7 and hour < 10:\n",
    "            self.ipda_true_day_time = time_ny_open\n",
    "        if hour >= 10 and hour < 12:\n",
    "            self.ipda_true_day_time = time_london_close\n",
    "        if hour >= 12 and hour < 20:\n",
    "            self.ipda_true_day_time = time_outside\n",
    "        if hour >= 20 and hour < 24:\n",
    "            self.ipda_true_day_time = time_asian_range\n",
    "            \n",
    "            \n",
    "    \n",
    "    ipda_true_day_time = 0\n",
    "    \n",
    "    buyside_taken = False\n",
    "    sellside_taken = False\n",
    "    swing_l_h = [False, False]\n",
    "    \n",
    "    action = 0\n",
    "    \n",
    "    def add_action(self, index):\n",
    "        self.action[index] = 1\n",
    "        \n",
    "###  gets the candlestick data using get_prices() and returns pd arrays + candles on update()\n",
    "class timeframechart:\n",
    "    candles = []\n",
    "    pd_arrays = []\n",
    "    \n",
    "    def __init__(self, symbol, timeframe):\n",
    "        self.symbol = symbol\n",
    "        self.timeframe = timeframe\n",
    "        \n",
    "    def reset(self):\n",
    "        self.candles = []\n",
    "        self.pd_arrays = []\n",
    "        self.liquidity_pools = []\n",
    "        \n",
    "    def update(self):\n",
    "        self.reset()\n",
    "        prices = get_prices(self.symbol, self.timeframe)\n",
    "        for i in prices.iloc:\n",
    "            c = candle(list(i))\n",
    "            self.candles.append(c)\n",
    "        return self.process_candles()\n",
    "        \n",
    "    \n",
    "    def detect_swing_point(self, current_candles):\n",
    "        swing_high = False\n",
    "        swing_low = False\n",
    "        if current_candles[-1].h<current_candles[-2].h and current_candles[-3].h<current_candles[-2].h:\n",
    "            swing_high = True\n",
    "        if current_candles[-1].l>current_candles[-2].l and current_candles[-3].l>current_candles[-2].l:\n",
    "            swing_low = True\n",
    "        return swing_low, swing_high\n",
    "        \n",
    "    liquidity_pools = []\n",
    "    def process_candles(self):\n",
    "        current_candles = []\n",
    "        last_liquidity_taken = 0\n",
    "        \n",
    "        for i in self.candles:\n",
    "            current_candles.append(i)\n",
    "            \n",
    "            ###################swing points / liquidity buildup\n",
    "            if len(current_candles) > 3:\n",
    "                swing_l_h = self.detect_swing_point(current_candles)\n",
    "                current_candles[-2].swing_l_h = swing_l_h\n",
    "                \n",
    "                if current_candles[-2].swing_l_h[0]:\n",
    "                    self.liquidity_pools.append([current_candles[-2].l, \"sellside\", len(current_candles)-2, False])\n",
    "                    current_candles[-2].add_action(action_swing_low_created)\n",
    "                    \n",
    "                if current_candles[-2].swing_l_h[1]:\n",
    "                    self.liquidity_pools.append([current_candles[-2].h, \"buyside\", len(current_candles)-2, False])\n",
    "                    current_candles[-2].add_action(action_swing_high_created)\n",
    "                \n",
    "            current_candles[-1].swing_l_h = [False, False]\n",
    "            ###################swing points / liquidity buildup\n",
    "            \n",
    "            \n",
    "            ################################## liquidity runs\n",
    "            current_candles[-1].buyside_taken = False\n",
    "            current_candles[-1].sellside_taken = False\n",
    "            for i in range(len(self.liquidity_pools)):\n",
    "                if current_candles[-1].h > self.liquidity_pools[i][0] and self.liquidity_pools[i][1] == \"buyside\":\n",
    "                    current_candles[-1].buyside_taken = True\n",
    "                    current_candles[-1].add_action(action_buyside_taken)\n",
    "                    self.liquidity_pools[i][3] = True\n",
    "                    last_liquidity_taken = \"buyside\"\n",
    "                    \n",
    "                if current_candles[-1].l < self.liquidity_pools[i][0] and self.liquidity_pools[i][1] == \"sellside\":\n",
    "                    current_candles[-1].sellside_taken = True\n",
    "                    current_candles[-1].add_action(action_sellside_taken)\n",
    "                    self.liquidity_pools[i][3] = True\n",
    "                    last_liquidity_taken = \"sellside\"\n",
    "            \n",
    "            #delete liquidity taken\n",
    "            while True:\n",
    "                sth = False\n",
    "                for i in range(len(self.liquidity_pools)):\n",
    "                    if self.liquidity_pools[i][3] == True:\n",
    "                        del self.liquidity_pools[i]\n",
    "                        sth = True\n",
    "                        break\n",
    "                if not sth: break\n",
    "            ################################## liquidity runs\n",
    "            \n",
    "            \n",
    "        self.candles = current_candles\n",
    "        \n",
    "        \n",
    "        ################## pd arrays\n",
    "        for i in range(len(self.candles)):\n",
    "            ### pd arrays repriced\n",
    "            for o in range(len(self.pd_arrays)):\n",
    "                \n",
    "                #### fair value gap repricing\n",
    "                if type(self.pd_arrays[o]) == fair_value_gap:\n",
    "                    if self.pd_arrays[o].bull_bear == 1:\n",
    "                        if self.candles[i].l < self.pd_arrays[o].high:\n",
    "                            if self.pd_arrays[o].retraded == False:\n",
    "                                self.candles[i].add_action(action_bull_fvg_retraded)\n",
    "                                self.pd_arrays[o].retraded = True\n",
    "                            \n",
    "                            if self.candles[i].l < self.pd_arrays[o].low and self.pd_arrays[o].closed == False:\n",
    "                                self.candles[i].add_action(action_bull_fvg_closed)\n",
    "                                self.pd_arrays[o].closed = True\n",
    "                                \n",
    "                                \n",
    "                    if self.pd_arrays[o].bull_bear == -1:\n",
    "                        if self.candles[i].h > self.pd_arrays[o].low:\n",
    "                            if self.pd_arrays[o].retraded == False:\n",
    "                                self.candles[i].add_action(action_bear_fvg_retraded)\n",
    "                                self.pd_arrays[o].retraded = True\n",
    "                            \n",
    "                            if self.candles[i].h > self.pd_arrays[o].high and self.pd_arrays[o].closed == False:\n",
    "                                self.candles[i].add_action(action_bear_fvg_closed)\n",
    "                                self.pd_arrays[o].closed = True\n",
    "            \n",
    "            \n",
    "            \n",
    "            ### fair value gaps ###\n",
    "            if i > 1 and i + 1 < len(self.candles):\n",
    "                ## bull fvg\n",
    "                if self.candles[i-1].h < self.candles[i+1].l:\n",
    "                    fvg = fair_value_gap(1, self.candles[i-1].h, self.candles[i+1].l)\n",
    "                    self.pd_arrays.append(fvg)\n",
    "                    self.candles[i].add_action(action_fvg_bull_created)\n",
    "                \n",
    "                ## bear fvg\n",
    "                if self.candles[i-1].l > self.candles[i+1].h:\n",
    "                    fvg = fair_value_gap(-1, self.candles[i+1].h, self.candles[i-1].l)\n",
    "                    self.pd_arrays.append(fvg)\n",
    "                    self.candles[i].add_action(action_fvg_bear_created)\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        ######## xclosed candles broken\n",
    "        last_xclosed_candles = [0,0,0]\n",
    "        current_xclosed_candles = [0,0,0]\n",
    "        ob_created = False\n",
    "        for i in range(len(self.candles)):        \n",
    "            current_candle_close = 1 if self.candles[i].c > self.candles[i].o else -1\n",
    "            \n",
    "            if current_xclosed_candles[0] != current_candle_close:\n",
    "                last_xclosed_candles = current_xclosed_candles\n",
    "                current_xclosed_candles = [current_candle_close, 1000000,0]\n",
    "                ob_created = False\n",
    "                \n",
    "            current_xclosed_candles[1] = min(current_xclosed_candles[1], self.candles[i].l)\n",
    "            current_xclosed_candles[2] = max(current_xclosed_candles[2], self.candles[i].h)\n",
    "            \n",
    "            if last_xclosed_candles[0] == -1 and self.candles[i].c > last_xclosed_candles[2]:\n",
    "                #downclose broken\n",
    "                if not ob_created:\n",
    "                    ob = xclosedcandles_broken(1, last_xclosed_candles[1], last_xclosed_candles[2])\n",
    "                    ob_created = True\n",
    "                    self.pd_arrays.append(ob)\n",
    "                    self.candles[i].add_action(action_ob_bull_created)\n",
    "            \n",
    "            if last_xclosed_candles[0] == 1 and self.candles[i].c < last_xclosed_candles[1]:\n",
    "                #downclose broken\n",
    "                if not ob_created:\n",
    "                    ob = xclosedcandles_broken(-1, last_xclosed_candles[1], last_xclosed_candles[2])\n",
    "                    ob_created = True\n",
    "                    self.pd_arrays.append(ob)\n",
    "                    self.candles[i].add_action(action_ob_bear_created)\n",
    "                    \n",
    "        ######## xclosed candles broken\n",
    "            \n",
    "                    \n",
    "        ################## pd arrays\n",
    "        \n",
    "        ## append liquidity pools to pd_arrays\n",
    "        for i in self.liquidity_pools:\n",
    "            if i[1] == \"buyside\":\n",
    "                liquidity_pool = liquidity(i[0], 1)\n",
    "            if i[1] == \"sellside\":\n",
    "                liquidity_pool = liquidity(i[0], -1)\n",
    "            self.pd_arrays.append(liquidity_pool)\n",
    "        \n",
    "        #cv_plot_candles(self.candles, self.pd_arrays)\n",
    "        return self.candles, self.pd_arrays\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be58422-493c-4a0f-a964-9b87ad1840c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ta to model input arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbdcee6a-8a15-4587-940e-94eeadd103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_to_model_input(candles, pd_arrays):\n",
    "    action_buffer = deque(maxlen = maxlen)\n",
    "    pd_array_price_level = [0,0,0,0,0] ## liquidity (1/-1/0), fvg(1/-1/0), fvg_open/retraded/closed(1/-1/0), xclosed_candles_broken(1/0/-1), current price\n",
    "    pd_array_prices = np.array([pd_array_price_level for _ in range(res_high)]).reshape(res_high, 5)\n",
    "    action_state = [0 for _ in range(6)] + [0 for _ in range(12)] + [0] ## time(6) + actions(12) + price relative to range, close price\n",
    "    \n",
    "    maxprice = 0\n",
    "    minprice = 100000\n",
    "    \n",
    "    for i in candles:\n",
    "        if i.h > maxprice:\n",
    "            maxprice = i.h\n",
    "        if i.l < minprice:\n",
    "            minprice = i.l\n",
    "    hlrange = maxprice - minprice\n",
    "    \n",
    "    for i in candles:\n",
    "        if np.sum(i.action) != 0:\n",
    "            new_action_state = i.ipda_true_day_time + i.action + [(i.c - minprice) / hlrange*2-1]\n",
    "            action_buffer.append(new_action_state)\n",
    "            \n",
    "    scaled_price = int((candles[-1].c - minprice) / hlrange * (res_high-1))\n",
    "    for i in pd_arrays:\n",
    "        if type(i) == liquidity:\n",
    "            b_s = i.l_type\n",
    "            price = i.price\n",
    "            scaled = int((price - minprice) / hlrange * (res_high-1))\n",
    "            pd_array_prices[scaled][0] = b_s\n",
    "            \n",
    "            \n",
    "        \n",
    "        if type(i) == fair_value_gap:\n",
    "            direction = i.bull_bear\n",
    "            fvg_status = 1 # open\n",
    "            if i.retraded == True:\n",
    "                fvg_status = -1\n",
    "            if i.closed == True:\n",
    "                fvg_status = 0\n",
    "            low_scaled = int((i.low - minprice) / hlrange * (res_high-1))\n",
    "            high_scaled = int((i.high - minprice) / hlrange * (res_high-1))\n",
    "            \n",
    "            for o in range(low_scaled, high_scaled+1):\n",
    "                pd_array_prices[o][1] = direction\n",
    "                pd_array_prices[o][2] = fvg_status\n",
    "                \n",
    "            \n",
    "            \n",
    "        if type(i) == xclosedcandles_broken:\n",
    "            direction = i.bull_bear\n",
    "            \n",
    "            low_scaled = int((i.low - minprice) / hlrange * (res_high-1))\n",
    "            high_scaled = int((i.high - minprice) / hlrange * (res_high-1))\n",
    "            \n",
    "            for o in range(low_scaled, high_scaled+1):\n",
    "                pd_array_prices[o][3] = direction\n",
    "                \n",
    "    return np.array(action_buffer), pd_array_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249a1c4-38b0-4e69-9421-8cb97e8db9a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## trade manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8ec8640-3180-4657-9329-a3d9b06f0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class order_class:\n",
    "    limit = 0\n",
    "    vol = 0\n",
    "    def __init__(self, price, vol):\n",
    "        self.limit = price\n",
    "        self.vol = vol\n",
    "    \n",
    "class position:\n",
    "    def __init__(self, entry_price, vol):\n",
    "        self.entry_price = entry_price\n",
    "        self.vol = vol\n",
    "        self.exit_price = -1\n",
    "        \n",
    "    def update_profit(self, price, high, low):\n",
    "        if self.exit_price != -1:\n",
    "            if (self.vol < 0 and low < self.exit_price) or (self.vol > 0 and high > self.exit_price):\n",
    "                self.profit = (self.exit_price - self.entry_price) * self.vol * 100000 \n",
    "                #print(\"close\", self.vol, \"at\", self.exit_price, \"for profit of\", self.profit)\n",
    "                self.closed = True\n",
    "        \n",
    "        self.profit = (price - self.entry_price) * self.vol * 100000 \n",
    "        \n",
    "    profit = 0\n",
    "    vol = 0\n",
    "    entry_price = 0\n",
    "    exit_price = 0\n",
    "    closed = False\n",
    "\n",
    "class trade_manager:\n",
    "    def __init__(self):\n",
    "        self.mode = 0 # 1 = buy, -1 = sell, 0 = close only, 2 = both\n",
    "        self.orders = []\n",
    "        self.positions = []\n",
    "        self.balance = 0\n",
    "        self.equity = 0\n",
    "        self.net_position = 0\n",
    "        \n",
    "    mode = 0 # 1 = buy, -1 = sell, 0 = close only\n",
    "    orders = []\n",
    "    positions = []\n",
    "    balance = 0\n",
    "    equity = 0\n",
    "    \n",
    "    def cv2_plot(self):\n",
    "        w = 1000\n",
    "        h = 400\n",
    "        canvas = np.zeros((h,w,3), np.uint8) \n",
    "        l = len(self.candles)\n",
    "        single_candle_w = w / l * 0.95\n",
    "        max_h = 0\n",
    "        max_l = 1000000\n",
    "        for i in self.candles:\n",
    "            if i.h > max_h:\n",
    "                max_h = i.h\n",
    "            if i.l < max_l:\n",
    "                max_l = i.l\n",
    "        hlrange = max_h - max_l\n",
    "        def scale_p(p):\n",
    "            return (p - max_l) / hlrange * h\n",
    "    \n",
    "        for i in range(len(self.candles)):\n",
    "            color = (0,200,0) if self.candles[i].c > self.candles[i].o else (0,0,200)\n",
    "            cv2.rectangle(canvas, (int(i*single_candle_w),int(scale_p(self.candles[i].o))), (int((i+1)*single_candle_w),int(scale_p(self.candles[i].c))), color, -1)\n",
    "            cv2.line(canvas, (int((i+0.5)*single_candle_w),int(scale_p(self.candles[i].h))), (int((i+0.5)*single_candle_w),int(scale_p(self.candles[i].l))), color)\n",
    "            \n",
    "        for i in self.orders:\n",
    "            if i.vol > 0:\n",
    "                cv2.line(canvas, (0,int(scale_p(i.limit))), (w,int(scale_p(i.limit))), (0,100,0), 1)\n",
    "            else:\n",
    "                cv2.line(canvas, (0,int(scale_p(i.limit))), (w,int(scale_p(i.limit))), (0,0,100), 1)\n",
    "                \n",
    "        for i in self.positions:\n",
    "            if i.vol > 0:\n",
    "                cv2.line(canvas, (0,int(scale_p(i.entry_price))), (w,int(scale_p(i.entry_price))), (0,255,0), 1)\n",
    "                if i.exit_price != -1:\n",
    "                    cv2.line(canvas, (0,int(scale_p(i.exit_price))), (w,int(scale_p(i.exit_price))), (100,0,100), 1)\n",
    "            else:\n",
    "                cv2.line(canvas, (0,int(scale_p(i.entry_price))), (w,int(scale_p(i.entry_price))), (0,0,255), 1)\n",
    "                if i.exit_price != -1:\n",
    "                    cv2.line(canvas, (0,int(scale_p(i.exit_price))), (w,int(scale_p(i.exit_price))), (100,100,0), 1)\n",
    "        \n",
    "        color = (0,200,0) if self.net_position > 0 else (0,0,200)\n",
    "        cv2.rectangle(canvas, (0,0), (int(abs(self.net_position)*1000), 50), color, -1)\n",
    "        \n",
    "        \n",
    "        canvas = np.flipud(canvas)\n",
    "        \n",
    "        canvas2 = np.zeros((h,w,3), np.uint8) \n",
    "        cv2.putText(canvas2,str(round(self.equity,2)), (int(w/3),h-50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255))\n",
    "        cv2.putText(canvas2,\"Mode: \" +  str(self.mode), (int(2*w/3),h-50), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255))\n",
    "        \n",
    "        canvas = canvas + canvas2\n",
    "        \n",
    "        cv2.imshow(\"trademanager\", canvas)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "    def update(self, new_mode, candles, pd_arrays):\n",
    "        self.candles = candles\n",
    "        self.mode = new_mode\n",
    "        while len(self.orders) > 0:\n",
    "            order = self.orders[0]\n",
    "            if order.vol < 0: #sell\n",
    "                if candles[-1].h > order.limit:\n",
    "                    p = position(order.limit, order.vol)\n",
    "                    self.positions.append(p)\n",
    "                    self.balance -= commission * order.vol*100000\n",
    "                    #print(\"sell\", order.vol, \"at\", order.limit)\n",
    "            if order.vol > 0: #buy\n",
    "                if candles[-1].l < order.limit:\n",
    "                    p = position(order.limit, order.vol)\n",
    "                    self.positions.append(p)\n",
    "                    self.balance -= commission * order.vol*100000\n",
    "                    #print(\"buy\", order.vol, \"at\", order.limit)\n",
    "            del self.orders[0]\n",
    "        \n",
    "        self.equity = self.balance\n",
    "        for i in range(len(self.positions)):\n",
    "            self.positions[i].update_profit(candles[-1].c, candles[-1].h, candles[-1].l)\n",
    "            self.equity += self.positions[i].profit\n",
    "            \n",
    "        while True:\n",
    "            sth = False\n",
    "            for i in range(len(self.positions)):\n",
    "                if self.positions[i].closed:\n",
    "                    self.balance+=self.positions[i].profit\n",
    "                    del self.positions[i]\n",
    "                    sth = True\n",
    "                    break\n",
    "            if not sth:break\n",
    "            \n",
    "        for i in range(len(self.positions)):\n",
    "            self.positions[i].exit_price = -1\n",
    "            \n",
    "            \n",
    "        ### close all when mode == 0\n",
    "#        if self.mode == 0:\n",
    "#            while len(self.positions) > 0:\n",
    "#                self.balance+=self.positions[0].profit\n",
    "#                del self.positions[0]\n",
    "#        else:                \n",
    "        \n",
    "            \n",
    "        ### update positions exit and add new orders\n",
    "        for i in pd_arrays[::-1]:\n",
    "                if type(i) == liquidity:\n",
    "                    \n",
    "                    if i.l_type == 1: ##buyside\n",
    "                        for o in range(len(self.positions)):\n",
    "                            #if (self.positions[o].exit_price == -1 or self.positions[o].exit_price > i.price) and self.positions[o].vol > 0:\n",
    "                            if self.positions[o].exit_price == -1 and self.positions[o].vol > 0:\n",
    "                                self.positions[o].exit_price = i.price * offset_limit\n",
    "                                if self.mode != -1:\n",
    "                                    break\n",
    "                        if self.mode == -1 or self.mode == 2:\n",
    "                            for _ in range(num_open):\n",
    "                                if -net_position_max_th < self.net_position:\n",
    "                                    o = order_class(i.price * offset_limit, -position_vol)\n",
    "                                    self.orders.append(o)\n",
    "                            \n",
    "                    if i.l_type == -1: ##sellside\n",
    "                        for o in range(len(self.positions)):\n",
    "                            #if (self.positions[o].exit_price == -1 or self.positions[o].exit_price < i.price) and self.positions[o].vol < 0:\n",
    "                            if self.positions[o].exit_price == -1 and self.positions[o].vol < 0:\n",
    "                                self.positions[o].exit_price = i.price / offset_limit\n",
    "                                if self.mode != 1:\n",
    "                                    break\n",
    "                        \n",
    "                        if self.mode == 1 or self.mode == 2:\n",
    "                            for _ in range(num_open):\n",
    "                                if net_position_max_th > self.net_position:\n",
    "                                    o = order_class(i.price / offset_limit, position_vol)\n",
    "                                    self.orders.append(o)\n",
    "        \n",
    "        self.net_position = 0\n",
    "        for i in self.positions:\n",
    "            self.net_position+=i.vol\n",
    "        \n",
    "        \n",
    "        ### plot with cv2\n",
    "        \n",
    "        self.cv2_plot()\n",
    "        \n",
    "        \n",
    "        return self.orders, self.positions, self.balance, self.equity, self.net_position\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f60850-4189-4b5b-9952-6650ffc45aa0",
   "metadata": {},
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed6cd575-a3e0-4b79-940b-30ba60349963",
   "metadata": {},
   "outputs": [],
   "source": [
    "candles_dir = \"archive/\"\n",
    "candles_files = [\"15_EURUSD.csv\", \"15_GBPUSD.csv\"]\n",
    "\n",
    "\n",
    "class environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.t = timeframechart(\"params only in live mode\", \"...\") # in training mode get_prices() is overwritten\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    dindex = lookback\n",
    "    def reset(self):\n",
    "        use_file = candles_dir+random.choice(candles_files)\n",
    "        print(use_file)\n",
    "        self.tm = trade_manager()\n",
    "        self.df = pd.read_csv(use_file)\n",
    "        self.dindex = lookback\n",
    "        self.update_prices()\n",
    "        \n",
    "        self.equity = 0\n",
    "        self.equity = 0\n",
    "        \n",
    "        self.candles, self.pd_arrays = self.t.update()\n",
    "        action_buffer, pd_matrix = chart_to_model_input(self.candles, self.pd_arrays)\n",
    "        return action_buffer, pd_matrix, 0\n",
    "    \n",
    "    def update_prices(self):\n",
    "        global prices\n",
    "        prices = self.df.iloc[self.dindex-lookback:self.dindex]\n",
    "        self.dindex += 1 \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def step(self, action): # 0 = buy, 1 = sell, 2 = close only\n",
    "        tmaction = \"\"\n",
    "        if action == 0:\n",
    "            tmaction = 1\n",
    "        if action == 1:\n",
    "            tmaction = -1\n",
    "        if action == 2:\n",
    "            tmaction = 0\n",
    "            \n",
    "        self.update_prices()\n",
    "        \n",
    "        \n",
    "        ### get new price data from chart\n",
    "        self.candles, self.pd_arrays = self.t.update()\n",
    "        orders, positions, b, e, n = self.tm.update(tmaction,self.candles,self.pd_arrays)\n",
    "        ### get new price data from chart\n",
    "        \n",
    "        ### compute next observation / model input\n",
    "        action_buffer, pd_matrix = chart_to_model_input(self.candles, self.pd_arrays)\n",
    "        ### compute next observation / model input\n",
    "        \n",
    "        \n",
    "        ### compute reward\n",
    "        self.last_equity = self.equity\n",
    "        self.equity = e\n",
    "        reward = self.equity - self.last_equity\n",
    "        \n",
    "        if abs(n) > net_position_pen_th:\n",
    "            too_much = abs(n) - net_position_pen_th\n",
    "            reward -= net_position_pen_th_value * too_much\n",
    "            #print(\"\\n\",n, net_position_pen_th, too_much)\n",
    "            #input()\n",
    "            \n",
    "            \n",
    "        ### compute reward\n",
    "        \n",
    "        done = self.dindex >= len(self.df)\n",
    "        \n",
    "        observation = np.array(action_buffer), np.array(pd_matrix), np.array([n/net_position_pen_th*10])\n",
    "        \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3dbcf-fb1f-40e4-a209-a8372d47dfba",
   "metadata": {},
   "source": [
    "## dqn agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e783a37-efaa-4e66-9344-0ffcb3c0696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 100000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 1000,\n",
    "                 exploration = 0.01\n",
    "                ):\n",
    "        self.exploration = exploration\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_sync = target_model_sync\n",
    "   \n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "      \n",
    "    \n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name, overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def model_call(self, x):\n",
    "        x1, x2, x3 = x\n",
    "        return tf.math.argmax(self.model([x1,x2,x3]), axis = 1)[0]\n",
    "    \n",
    "    def select_action(self, state1, state2, state3):\n",
    "        if np.random.random() < self.exploration: # random action\n",
    "            return np.random.randint(0,self.n_actions)\n",
    "        \n",
    "        ret = self.model_call([state1, state2, state3])\n",
    "        return ret\n",
    "\n",
    "\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state])\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.target_model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "\n",
    "        \n",
    "    #@tf.function(jit_compile = False)\n",
    "    def tstep(self, data):\n",
    "        states, next_states, rewards, terminals, masks = data\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            model_return = self.model(states, training=True) \n",
    "            mask_return = model_return * masks\n",
    "            estimated_q_values = tf.math.reduce_sum(mask_return, axis=1)\n",
    "            #print(estimated_q_values, mask_return, model_return, masks)\n",
    "            loss_e = tf.math.square(target_q_values - estimated_q_values)\n",
    "            loss = tf.reduce_mean(loss_e)\n",
    "        \n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values)\n",
    "    \n",
    "    \n",
    "    def data_get_func(self):\n",
    "        idx = np.random.randint(0, len(self.memory), self.batch_size)\n",
    "        sarts_batch = [self.memory[i] for i in idx]\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_1 = np.array([x[0] for x in states], dtype=\"float32\")\n",
    "        states_2 = np.array([x[1] for x in states], dtype=\"float32\")\n",
    "        states_3 = np.array([x[2] for x in states], dtype=\"float32\")\n",
    "        \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_1 = np.array([x[0] for x in next_states], dtype=\"float32\")\n",
    "        next_states_2 = np.array([x[1] for x in next_states], dtype=\"float32\")\n",
    "        next_states_3 = np.array([x[2] for x in next_states], dtype=\"float32\")\n",
    "        \n",
    "        masks = np.array(self.m1[actions])\n",
    "        return [states_1, states_2, states_3], [next_states_1, next_states_2, next_states_3], rewards, terminals, masks\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.total_steps_trained+=1\n",
    "        if self.total_steps_trained % self.target_model_sync == 0:\n",
    "            self.copy_weights()\n",
    "\n",
    "           \n",
    "        data = self.data_get_func()\n",
    "        result= self.tstep(data)\n",
    "   \n",
    "        return  result\n",
    "    \n",
    "    \n",
    "    def train(self, num_steps, env, log_interval = 1000, warmup = 0, train_steps_per_step = 1):\n",
    "        self.total_steps_trained = -1\n",
    "\n",
    "        \n",
    "        states = x.reset()\n",
    "        \n",
    "        current_episode_reward_sum = 0\n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "\n",
    "        self.total_rewards = []\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        \n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            file = open(log_folder+\"logs/loss_log.txt\", \"a\")  \n",
    "            #for loss in self.losses:\n",
    "                        #file.write(str(loss))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.losses)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/qv_log.txt\", \"a\")  \n",
    "            #for qv in self.q_v:\n",
    "                        #file.write(str(qv))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.q_v)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/rewards_log.txt\", \"a\")  \n",
    "            #for total_reward in self.total_rewards:\n",
    "                        #file.write(str(total_reward))\n",
    "                        #file.write(\"\\n\")\n",
    "                    \n",
    "            file.write(str(np.mean(self.total_rewards)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.total_rewards = []\n",
    "            self.losses = [0]\n",
    "            self.q_v = [0]\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.05, stateful_metrics = [\"reward sum\", \"t\"])\n",
    "\n",
    "\n",
    "                state_1 = np.array([states[0]])\n",
    "                state_2 = np.array([states[1]])\n",
    "                state_3 = np.array([states[2]])\n",
    "                \n",
    "                action = self.select_action(state_1, state_2, state_3)\n",
    "#                print(action)\n",
    "\n",
    "                next_states, reward, done = env.step(action)\n",
    "                \n",
    "                current_episode_reward_sum += reward\n",
    "\n",
    "                self.total_rewards.append(reward)\n",
    "\n",
    "                if done == True:\n",
    "                        next_states = env.reset()\n",
    "                self.observe_sasrt(states, action, next_states, reward, done)\n",
    "\n",
    "                states = next_states\n",
    "                if i > warmup:\n",
    "                    for _ in range(train_steps_per_step):\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "                if (i+1) % log_interval == 0:\n",
    "                    #print(\"-----------\")\n",
    "                    #print(\"l:\", np.mean(self.losses))\n",
    "                    #print(\"q:\", np.mean(self.q_v))\n",
    "                    #print(\"reward sum\", current_episode_reward_sum)\n",
    "                    #print(\"t\", np.mean(times))\n",
    "                    #print(\"-----------\")\n",
    "                    save_current_run()\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-train_steps_per_step:])),\n",
    "                                (\"mean q\", np.mean(self.q_v[-train_steps_per_step:])),\n",
    "                                (\"rewards\", np.mean(reward)),\n",
    "                                (\"reward sum\", current_episode_reward_sum),\n",
    "                                (\"t\", np.mean(times))])\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68d77a-f557-43c2-b8d5-9b942959ccc4",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e164c02c-4db3-432a-9bfe-91bbdbd32539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 384, 19)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 384, 32)      1856        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 384, 51)      0           conv1d[0][0]                     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 384, 32)      1664        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128, 5)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 384, 32)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 32)      512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 384, 32)      64          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 37)      0           conv1d_1[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 384, 128)     82432       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128, 32)      1216        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 384, 128)     131584      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128, 32)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 384, 128)     131584      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 128, 32)      64          leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 384, 64)      8256        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 32)      3104        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 384, 64)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 37)      0           conv1d_2[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 384, 32)      2080        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128, 32)      1216        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 384, 32)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128, 32)      0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 384, 16)      528         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128, 8)       264         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 384, 16)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128, 8)       0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 384, 16)      32          leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 128, 8)       16          leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6144)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7169)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         7342080     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         1049600     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            3072        leaky_re_lu_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,810,824\n",
      "Trainable params: 9,810,824\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if True:\n",
    "    input_action_buffer = tf.keras.layers.Input(shape = (maxlen, 19))\n",
    "    input_pd_matrix = tf.keras.layers.Input(shape = (res_high, 5))\n",
    "    input_net_position = tf.keras.layers.Input(shape = (1))\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(32, 3,activation=\"relu\", padding=\"same\")(input_action_buffer)\n",
    "    x1 = tf.keras.layers.Concatenate()([x1,input_action_buffer])\n",
    "\n",
    "    x1 = tf.keras.layers.Dense(32)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    \n",
    "    x1 = tf.keras.layers.LSTM(128,return_sequences = True)(x1)\n",
    "    x1 = tf.keras.layers.LSTM(128,return_sequences = True)(x1)\n",
    "    x1 = tf.keras.layers.LSTM(128,return_sequences = True)(x1)\n",
    "        \n",
    "    x1 = tf.keras.layers.Dense(64)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(32)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(16)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "    x2 = tf.keras.layers.Conv1D(32, 3,activation=\"relu\", padding=\"same\")(input_pd_matrix)\n",
    "    x2 = tf.keras.layers.Concatenate()([x2,input_pd_matrix])\n",
    "    \n",
    "    x2 = tf.keras.layers.Dense(32)(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.LayerNormalization()(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Conv1D(32, 3,activation=\"relu\", padding=\"same\")(x2)\n",
    "    x2 = tf.keras.layers.Concatenate()([x2,input_pd_matrix])\n",
    "\n",
    "    x2 = tf.keras.layers.Dense(32)(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(8)(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.LayerNormalization()(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([x1,x2,input_net_position])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(3, activation = \"linear\", use_bias=False, dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model([input_action_buffer,input_pd_matrix,input_net_position], outputs)\n",
    "    \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe772a-67dd-4e6b-b7b8-da7d404433b9",
   "metadata": {},
   "source": [
    "## running it all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "552abad3-d51a-4af9-9103-96c0e483bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights...\n"
     ]
    }
   ],
   "source": [
    "resume = True\n",
    "opt = tf.keras.optimizers.Adam(0.00002)\n",
    "\n",
    "name = \"dqn_trading\"\n",
    "log_folder = \"./\"\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model = model, \n",
    "    n_actions = 3, \n",
    "    memory_size = 100000, \n",
    "    gamma=0.95,\n",
    "    optimizer = opt,\n",
    "    batch_size = 128, \n",
    "    target_model_sync = 500,\n",
    "    exploration = 0.02,\n",
    "    name=log_folder+name+\".h5\")\n",
    "\n",
    "if resume:\n",
    "\tprint(\"loading weights...\")\n",
    "\tagent.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "794aea76-372f-4d5e-8a2f-fe122f5b0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/15_GBPUSD.csv\n",
      "warmup...\n",
      "archive/15_GBPUSD.csv\n",
      "1000/1000 [==============================] - 364s 363ms/step - loss: 0.0000e+00 - mean q: 0.0000e+00 - rewards: -0.0326 - reward sum: -32.6128 - t: 305.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\root\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\root\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "x = environment()\n",
    "print(\"warmup...\")\n",
    "n = 1000\n",
    "agent.train(num_steps = n, env = x, warmup = n, log_interval = n, train_steps_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db362a0b-91d5-4308-9a80-a87ec585be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/15_GBPUSD.csv\n",
      "training...\n",
      "archive/15_EURUSD.csv\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 645.5222 - mean q: 105.2224 - rewards: -0.2080 - reward sum: -207.9793 - t: 539.6514\n",
      "1000/1000 [==============================] - 540s 540ms/step - loss: 324.8062 - mean q: 75.7302 - rewards: 0.1799 - reward sum: -28.0586 - t: 536.4099\n",
      "1000/1000 [==============================] - 546s 546ms/step - loss: 226.1211 - mean q: 65.1738 - rewards: -6.9202e-04 - reward sum: -28.7507 - t: 549.5249\n",
      "1000/1000 [==============================] - 555s 555ms/step - loss: 212.3994 - mean q: 57.6832 - rewards: -0.1343 - reward sum: -163.0926 - t: 585.3741\n",
      "1000/1000 [==============================] - 574s 574ms/step - loss: 195.7147 - mean q: 54.5695 - rewards: -0.5365 - reward sum: -699.5859 - t: 562.7849\n",
      "1000/1000 [==============================] - 537s 537ms/step - loss: 197.2174 - mean q: 51.9443 - rewards: 0.0097 - reward sum: -689.8833 - t: 532.3392\n",
      "1000/1000 [==============================] - 540s 540ms/step - loss: 178.4852 - mean q: 47.9732 - rewards: -0.3178 - reward sum: -1007.7306 - t: 528.1621\n",
      "1000/1000 [==============================] - 536s 536ms/step - loss: 170.4120 - mean q: 46.4378 - rewards: 0.8521 - reward sum: -155.5868 - t: 525.6191\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 169.1097 - mean q: 43.9640 - rewards: -0.4623 - reward sum: -617.9010 - t: 530.2347\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 223.2429 - mean q: 46.1518 - rewards: -0.9019 - reward sum: -1519.8064 - t: 532.3759\n",
      "1000/1000 [==============================] - 557s 557ms/step - loss: 209.3269 - mean q: 44.8253 - rewards: -0.5100 - reward sum: -2029.8499 - t: 528.8678\n",
      "1000/1000 [==============================] - 545s 545ms/step - loss: 203.5795 - mean q: 43.5112 - rewards: -0.0015 - reward sum: -2031.3850 - t: 561.8763\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 198.2101 - mean q: 44.0087 - rewards: -1.8606 - reward sum: -3892.0163 - t: 538.2905\n",
      "1000/1000 [==============================] - 542s 542ms/step - loss: 200.2603 - mean q: 42.6294 - rewards: 0.8278 - reward sum: -3064.2298 - t: 546.3853\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 190.3743 - mean q: 41.2769 - rewards: 0.0525 - reward sum: -3011.6981 - t: 544.7910\n",
      "1000/1000 [==============================] - 541s 541ms/step - loss: 182.7569 - mean q: 40.7934 - rewards: 0.1869 - reward sum: -2824.7847 - t: 534.1258\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 177.2777 - mean q: 39.6460 - rewards: -0.8633 - reward sum: -3688.0556 - t: 527.2258\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 166.2747 - mean q: 39.9751 - rewards: 0.9244 - reward sum: -2763.6832 - t: 532.7768\n",
      "1000/1000 [==============================] - 536s 536ms/step - loss: 161.4841 - mean q: 38.4292 - rewards: 0.1313 - reward sum: -2632.4049 - t: 537.5908\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 159.3835 - mean q: 40.6045 - rewards: 0.0731 - reward sum: -2559.2667 - t: 540.5332\n",
      "1000/1000 [==============================] - 521s 521ms/step - loss: 160.4871 - mean q: 37.6198 - rewards: 0.8933 - reward sum: -1665.9830 - t: 514.0016\n",
      "1000/1000 [==============================] - 510s 510ms/step - loss: 154.5849 - mean q: 39.4817 - rewards: 0.3839 - reward sum: -1282.0561 - t: 512.4279\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 162.2260 - mean q: 40.5617 - rewards: -0.2410 - reward sum: -1523.1054 - t: 533.8415\n",
      "1000/1000 [==============================] - 541s 541ms/step - loss: 171.3367 - mean q: 42.8696 - rewards: -0.4765 - reward sum: -1999.5662 - t: 519.6910\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 159.3418 - mean q: 42.1457 - rewards: -0.1879 - reward sum: -2187.4852 - t: 528.0422\n",
      "1000/1000 [==============================] - 533s 533ms/step - loss: 147.6017 - mean q: 43.8440 - rewards: -0.5803 - reward sum: -2767.7450 - t: 531.1751\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 145.5419 - mean q: 42.4577 - rewards: 0.2963 - reward sum: -2471.4686 - t: 515.5328\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 139.6062 - mean q: 43.5856 - rewards: 0.1604 - reward sum: -2311.0837 - t: 539.9916\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 138.7770 - mean q: 43.6693 - rewards: 0.7799 - reward sum: -1531.1636 - t: 549.3520\n",
      "1000/1000 [==============================] - 541s 541ms/step - loss: 5523.0036 - mean q: 44.9395 - rewards: -1.6406 - reward sum: -3171.7317 - t: 541.2983\n",
      "1000/1000 [==============================] - 548s 548ms/step - loss: 1545.4699 - mean q: 46.9487 - rewards: 0.3273 - reward sum: -2844.4218 - t: 533.2591\n",
      "1000/1000 [==============================] - 548s 548ms/step - loss: 500.8485 - mean q: 48.5001 - rewards: 1.6440 - reward sum: -1200.4138 - t: 537.7031\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 369.2996 - mean q: 48.4672 - rewards: -0.4796 - reward sum: -1680.0200 - t: 532.1698\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 335.5751 - mean q: 48.8243 - rewards: -0.9954 - reward sum: -2675.4698 - t: 517.5602\n",
      "1000/1000 [==============================] - 518s 518ms/step - loss: 287.9211 - mean q: 50.5148 - rewards: 1.4015 - reward sum: -1273.9707 - t: 505.1110\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 242.1468 - mean q: 52.3151 - rewards: -0.4669 - reward sum: -1740.8627 - t: 511.4514\n",
      "1000/1000 [==============================] - 511s 511ms/step - loss: 256.7625 - mean q: 53.3159 - rewards: 0.4609 - reward sum: -1279.9844 - t: 510.3232\n",
      "1000/1000 [==============================] - 516s 516ms/step - loss: 274.2051 - mean q: 56.1406 - rewards: -0.5128 - reward sum: -1792.7404 - t: 521.4033\n",
      "1000/1000 [==============================] - 524s 524ms/step - loss: 228.0682 - mean q: 59.4276 - rewards: -0.6058 - reward sum: -2398.5592 - t: 550.6573\n",
      "1000/1000 [==============================] - 544s 544ms/step - loss: 389.1713 - mean q: 58.7181 - rewards: -1.0411 - reward sum: -3439.6155 - t: 554.7730\n",
      "1000/1000 [==============================] - 563s 563ms/step - loss: 206.0175 - mean q: 57.2346 - rewards: 0.0810 - reward sum: -3358.5999 - t: 560.4605\n",
      "1000/1000 [==============================] - 595s 595ms/step - loss: 219.1118 - mean q: 56.5847 - rewards: -0.9001 - reward sum: -4258.6704 - t: 582.0404\n",
      "1000/1000 [==============================] - 584s 584ms/step - loss: 236.0261 - mean q: 54.7796 - rewards: -0.1779 - reward sum: -4436.5335 - t: 578.3649\n",
      "1000/1000 [==============================] - 575s 575ms/step - loss: 251.9700 - mean q: 55.7228 - rewards: 0.2754 - reward sum: -4161.1077 - t: 544.1769\n",
      "1000/1000 [==============================] - 555s 555ms/step - loss: 275.0950 - mean q: 55.3611 - rewards: -0.1750 - reward sum: -4336.1292 - t: 544.5103\n",
      "1000/1000 [==============================] - 544s 544ms/step - loss: 199.6268 - mean q: 54.7783 - rewards: -1.0109 - reward sum: -5347.0741 - t: 524.9255\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 368.1825 - mean q: 53.7308 - rewards: -0.1767 - reward sum: -5523.7784 - t: 525.8812\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 335.9436 - mean q: 57.1029 - rewards: -0.4282 - reward sum: -5951.9939 - t: 518.1155\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 176.7659 - mean q: 56.8256 - rewards: 0.0884 - reward sum: -5863.5580 - t: 521.4328\n",
      "1000/1000 [==============================] - 522s 522ms/step - loss: 337.0669 - mean q: 56.0338 - rewards: -0.8587 - reward sum: -6722.2118 - t: 521.5893\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 289.6210 - mean q: 59.2612 - rewards: 1.1207 - reward sum: -5601.4876 - t: 532.0114\n",
      "1000/1000 [==============================] - 523s 523ms/step - loss: 384.8521 - mean q: 58.8019 - rewards: -0.1793 - reward sum: -5780.7865 - t: 515.5151\n",
      "1000/1000 [==============================] - 549s 549ms/step - loss: 306.4060 - mean q: 57.8787 - rewards: -1.1664 - reward sum: -6947.1803 - t: 558.7810\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 522.0640 - mean q: 57.7550 - rewards: -0.9251 - reward sum: -7872.3224 - t: 531.6901\n",
      "1000/1000 [==============================] - 520s 520ms/step - loss: 265.6337 - mean q: 56.8130 - rewards: -0.1913 - reward sum: -8063.6573 - t: 512.2555\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 285.3332 - mean q: 55.2781 - rewards: 1.0050 - reward sum: -7058.6093 - t: 532.3264\n",
      "1000/1000 [==============================] - 539s 539ms/step - loss: 296.1156 - mean q: 54.6836 - rewards: -0.4946 - reward sum: -7553.2011 - t: 532.2227\n",
      "1000/1000 [==============================] - 556s 556ms/step - loss: 258.5487 - mean q: 53.9877 - rewards: -0.4942 - reward sum: -8047.3522 - t: 552.1686\n",
      "1000/1000 [==============================] - 535s 535ms/step - loss: 241.2099 - mean q: 52.4138 - rewards: 0.4553 - reward sum: -7592.0233 - t: 555.5156\n",
      "1000/1000 [==============================] - 536s 536ms/step - loss: 209.3463 - mean q: 52.6212 - rewards: -1.2987 - reward sum: -8890.7237 - t: 558.1915\n",
      "1000/1000 [==============================] - 539s 539ms/step - loss: 254.3779 - mean q: 53.8980 - rewards: -0.0453 - reward sum: -8936.0572 - t: 533.5887\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 326.5416 - mean q: 53.4600 - rewards: -1.9121 - reward sum: -10848.1878 - t: 545.3808\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 224.0987 - mean q: 52.9844 - rewards: -1.2864 - reward sum: -12134.6307 - t: 524.9688\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 245.8437 - mean q: 53.6318 - rewards: 1.2616 - reward sum: -10873.0346 - t: 543.8581\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 323.1658 - mean q: 51.2864 - rewards: -1.1285 - reward sum: -12001.5021 - t: 530.2293\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 377.6055 - mean q: 51.2998 - rewards: 0.1235 - reward sum: -11878.0485 - t: 530.5634\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 368.5487 - mean q: 53.4232 - rewards: 0.0320 - reward sum: -11846.0233 - t: 530.2897\n",
      "1000/1000 [==============================] - 538s 538ms/step - loss: 243.7454 - mean q: 52.1759 - rewards: -0.2543 - reward sum: -12100.3395 - t: 539.6858\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 229.1447 - mean q: 51.3573 - rewards: 0.1969 - reward sum: -11903.4042 - t: 534.3827\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 223.9124 - mean q: 50.0492 - rewards: -3.0137 - reward sum: -14917.0851 - t: 529.5496\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 280.7022 - mean q: 49.6728 - rewards: -0.5878 - reward sum: -15504.9198 - t: 530.0636\n",
      "1000/1000 [==============================] - 524s 524ms/step - loss: 229.3583 - mean q: 49.8927 - rewards: -2.2442 - reward sum: -17749.0706 - t: 532.6309\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 232.9385 - mean q: 50.3090 - rewards: 0.1933 - reward sum: -17555.7637 - t: 534.3840\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 621.5456 - mean q: 51.9679 - rewards: 0.1666 - reward sum: -17389.1520 - t: 518.5703\n",
      "1000/1000 [==============================] - 522s 522ms/step - loss: 224.2928 - mean q: 52.9953 - rewards: -1.3056 - reward sum: -18694.7450 - t: 521.7856\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 268.2902 - mean q: 54.6227 - rewards: -0.8815 - reward sum: -19576.2522 - t: 529.3786\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 270.0335 - mean q: 50.8123 - rewards: -0.8449 - reward sum: -20421.1393 - t: 526.8731\n",
      "1000/1000 [==============================] - 524s 524ms/step - loss: 354.4555 - mean q: 48.8668 - rewards: -0.9039 - reward sum: -21325.0231 - t: 520.1607\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 282.2956 - mean q: 48.4848 - rewards: 0.4591 - reward sum: -20865.9561 - t: 534.3789\n",
      "1000/1000 [==============================] - 532s 532ms/step - loss: 253.5844 - mean q: 51.1008 - rewards: -1.0037 - reward sum: -21869.6884 - t: 528.3290\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 262.7919 - mean q: 50.7365 - rewards: -0.2038 - reward sum: -22073.4968 - t: 528.5594\n",
      "1000/1000 [==============================] - 533s 533ms/step - loss: 283.6814 - mean q: 50.1429 - rewards: 0.1215 - reward sum: -21952.0397 - t: 531.0532\n",
      "1000/1000 [==============================] - 537s 537ms/step - loss: 200.0236 - mean q: 49.5491 - rewards: -0.0383 - reward sum: -21990.3355 - t: 552.3612\n",
      "1000/1000 [==============================] - 533s 533ms/step - loss: 229.3814 - mean q: 48.7885 - rewards: -0.7328 - reward sum: -22723.1089 - t: 530.5238\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 265.5099 - mean q: 47.7214 - rewards: 1.2718 - reward sum: -21451.3497 - t: 518.4261\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 217.1527 - mean q: 45.8619 - rewards: -0.2031 - reward sum: -21654.4706 - t: 537.7594\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 216.3548 - mean q: 47.1680 - rewards: -0.2157 - reward sum: -21870.2092 - t: 524.0827\n",
      "1000/1000 [==============================] - 521s 521ms/step - loss: 222.6805 - mean q: 45.6775 - rewards: 0.3660 - reward sum: -21504.1646 - t: 523.6999\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 211.5812 - mean q: 46.7429 - rewards: -0.0570 - reward sum: -21561.1954 - t: 529.0573\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 217.0445 - mean q: 45.8044 - rewards: -0.7373 - reward sum: -22298.4646 - t: 517.8669\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 242.6120 - mean q: 45.8402 - rewards: -0.1933 - reward sum: -22491.7513 - t: 514.0838\n",
      "1000/1000 [==============================] - 521s 521ms/step - loss: 220.4078 - mean q: 46.1962 - rewards: 0.6985 - reward sum: -21793.2451 - t: 545.4079\n",
      "1000/1000 [==============================] - 515s 515ms/step - loss: 247.1069 - mean q: 44.0704 - rewards: -0.2667 - reward sum: -22059.9604 - t: 506.1283\n",
      "1000/1000 [==============================] - 513s 513ms/step - loss: 239.9711 - mean q: 43.0409 - rewards: -0.0742 - reward sum: -22134.1815 - t: 518.9044\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 240.1726 - mean q: 42.3501 - rewards: 0.3305 - reward sum: -21803.6647 - t: 518.4530\n",
      "1000/1000 [==============================] - 518s 518ms/step - loss: 269.9383 - mean q: 43.2717 - rewards: -0.2149 - reward sum: -22018.5703 - t: 515.1278\n",
      "1000/1000 [==============================] - 520s 520ms/step - loss: 194.2667 - mean q: 43.5731 - rewards: -0.7906 - reward sum: -22809.1775 - t: 519.8392\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 209.0029 - mean q: 41.8543 - rewards: 0.8672 - reward sum: -21941.9504 - t: 517.9881\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 197.4125 - mean q: 40.3201 - rewards: -0.2308 - reward sum: -22172.7957 - t: 524.7567\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 272.1853 - mean q: 41.0650 - rewards: -0.3247 - reward sum: -22497.5319 - t: 528.8007\n",
      "1000/1000 [==============================] - 539s 539ms/step - loss: 262.8279 - mean q: 40.8769 - rewards: -2.4123 - reward sum: -24909.8253 - t: 540.3488\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 209.8595 - mean q: 42.5434 - rewards: -0.1370 - reward sum: -25046.8155 - t: 514.8603\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 199.7684 - mean q: 38.9078 - rewards: -0.3031 - reward sum: -25349.9346 - t: 540.1426\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 197.4438 - mean q: 39.1933 - rewards: 0.2054 - reward sum: -25144.5487 - t: 516.3621\n",
      "1000/1000 [==============================] - 522s 522ms/step - loss: 230.2359 - mean q: 38.6409 - rewards: -0.7117 - reward sum: -25856.2665 - t: 521.4166\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 217.3032 - mean q: 36.3727 - rewards: -1.4421 - reward sum: -27298.4155 - t: 523.4586\n",
      "1000/1000 [==============================] - 520s 520ms/step - loss: 222.1647 - mean q: 42.3103 - rewards: -0.4551 - reward sum: -27753.5271 - t: 515.8127\n",
      "1000/1000 [==============================] - 524s 524ms/step - loss: 218.8764 - mean q: 43.6080 - rewards: -0.8925 - reward sum: -28646.0093 - t: 514.3700\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 224.3599 - mean q: 45.1800 - rewards: 0.8422 - reward sum: -27803.8444 - t: 529.3843\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 197.0050 - mean q: 45.5254 - rewards: -0.9536 - reward sum: -28757.4085 - t: 530.6525\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 213.3460 - mean q: 45.8112 - rewards: -0.2771 - reward sum: -29034.4676 - t: 523.3453\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 203.1946 - mean q: 42.2814 - rewards: 0.3453 - reward sum: -28689.1923 - t: 529.4045\n",
      "1000/1000 [==============================] - 536s 536ms/step - loss: 215.0049 - mean q: 41.9780 - rewards: -0.0248 - reward sum: -28713.9746 - t: 539.3512\n",
      "1000/1000 [==============================] - 541s 541ms/step - loss: 198.6327 - mean q: 39.5783 - rewards: 1.1867 - reward sum: -27527.2688 - t: 534.0357\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 234.8879 - mean q: 40.8049 - rewards: 0.6898 - reward sum: -26837.4346 - t: 529.0234\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 221.0971 - mean q: 40.5429 - rewards: -0.2661 - reward sum: -27103.5051 - t: 530.9976\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 250.1102 - mean q: 38.4454 - rewards: -0.5621 - reward sum: -27665.5553 - t: 535.5932\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 206.4762 - mean q: 37.5890 - rewards: -0.1840 - reward sum: -27849.6035 - t: 522.9856\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 192.4732 - mean q: 38.5421 - rewards: -0.5053 - reward sum: -28354.8890 - t: 529.8763\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 246.1563 - mean q: 38.9121 - rewards: 0.2166 - reward sum: -28138.3122 - t: 527.0103\n",
      "1000/1000 [==============================] - 534s 534ms/step - loss: 278.0493 - mean q: 37.6823 - rewards: -0.0954 - reward sum: -28233.7395 - t: 528.8414\n",
      "1000/1000 [==============================] - 536s 536ms/step - loss: 217.2321 - mean q: 38.0617 - rewards: -0.0042 - reward sum: -28237.9702 - t: 531.2424\n",
      "1000/1000 [==============================] - 535s 535ms/step - loss: 188.1487 - mean q: 37.2844 - rewards: 0.1873 - reward sum: -28050.7136 - t: 534.5747\n",
      "1000/1000 [==============================] - 533s 533ms/step - loss: 306.7628 - mean q: 36.3986 - rewards: 0.3000 - reward sum: -27750.7356 - t: 527.3917\n",
      "1000/1000 [==============================] - 529s 529ms/step - loss: 279.3887 - mean q: 37.5837 - rewards: 0.2667 - reward sum: -27484.0435 - t: 546.7604\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 248.1933 - mean q: 38.9686 - rewards: 0.0808 - reward sum: -27403.2001 - t: 521.6305\n",
      "1000/1000 [==============================] - 533s 533ms/step - loss: 179.2856 - mean q: 39.1034 - rewards: -0.1735 - reward sum: -27576.6637 - t: 551.9060\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 197.1825 - mean q: 38.6325 - rewards: 0.0960 - reward sum: -27480.7136 - t: 526.5980\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 212.2442 - mean q: 38.0380 - rewards: 0.0992 - reward sum: -27381.5373 - t: 534.4267\n",
      "1000/1000 [==============================] - 530s 530ms/step - loss: 181.8343 - mean q: 38.0093 - rewards: 0.4697 - reward sum: -26911.7976 - t: 532.9073\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 183.6602 - mean q: 36.6817 - rewards: 0.0054 - reward sum: -26906.3640 - t: 517.3677\n",
      "1000/1000 [==============================] - 523s 523ms/step - loss: 181.0818 - mean q: 38.1850 - rewards: -0.2989 - reward sum: -27205.2658 - t: 536.9973\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 181.8556 - mean q: 39.7962 - rewards: -0.8633 - reward sum: -28068.5373 - t: 522.6085\n",
      "1000/1000 [==============================] - 523s 523ms/step - loss: 175.8935 - mean q: 39.1058 - rewards: 0.1043 - reward sum: -27964.2869 - t: 529.1090\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 167.8271 - mean q: 35.8673 - rewards: -0.6044 - reward sum: -28568.7125 - t: 523.7574\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 200.4545 - mean q: 35.9745 - rewards: -0.3710 - reward sum: -28939.6670 - t: 521.8351\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 173.1986 - mean q: 38.1724 - rewards: -0.0486 - reward sum: -28988.2570 - t: 561.8843\n",
      "1000/1000 [==============================] - 549s 549ms/step - loss: 175.0961 - mean q: 37.5445 - rewards: 0.2308 - reward sum: -28757.4524 - t: 555.1383\n",
      "1000/1000 [==============================] - 550s 550ms/step - loss: 200.1252 - mean q: 35.6796 - rewards: -0.1576 - reward sum: -28915.0523 - t: 570.6073\n",
      "1000/1000 [==============================] - 543s 543ms/step - loss: 184.8763 - mean q: 34.9413 - rewards: -1.5884 - reward sum: -30503.4831 - t: 515.3822\n",
      "1000/1000 [==============================] - 522s 522ms/step - loss: 191.5338 - mean q: 34.9859 - rewards: 0.7638 - reward sum: -29739.6536 - t: 518.2553\n",
      "1000/1000 [==============================] - 524s 524ms/step - loss: 169.1670 - mean q: 34.8588 - rewards: -0.3005 - reward sum: -30040.1661 - t: 520.1860\n",
      "1000/1000 [==============================] - 531s 531ms/step - loss: 173.7631 - mean q: 35.4930 - rewards: 0.6423 - reward sum: -29397.8548 - t: 533.6303\n",
      "1000/1000 [==============================] - 525s 524ms/step - loss: 164.1729 - mean q: 36.2147 - rewards: 0.2054 - reward sum: -29192.5021 - t: 523.0487\n",
      "1000/1000 [==============================] - 523s 523ms/step - loss: 187.8917 - mean q: 36.1656 - rewards: 0.3071 - reward sum: -28885.3783 - t: 515.8213\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 174.0796 - mean q: 36.3655 - rewards: -0.3401 - reward sum: -29225.5088 - t: 517.4198\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 177.8970 - mean q: 36.6960 - rewards: 0.3087 - reward sum: -28916.7987 - t: 519.0063\n",
      "1000/1000 [==============================] - 519s 519ms/step - loss: 162.1339 - mean q: 34.8918 - rewards: 0.0834 - reward sum: -28833.3695 - t: 514.5425\n",
      "1000/1000 [==============================] - 518s 518ms/step - loss: 165.5030 - mean q: 35.3313 - rewards: 0.2986 - reward sum: -28534.7658 - t: 514.6544\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 166.4018 - mean q: 35.6837 - rewards: -0.2602 - reward sum: -28795.0111 - t: 525.8723\n",
      "1000/1000 [==============================] - 528s 528ms/step - loss: 159.6929 - mean q: 33.8073 - rewards: -0.5560 - reward sum: -29350.9750 - t: 541.0944\n",
      "1000/1000 [==============================] - 535s 535ms/step - loss: 164.8260 - mean q: 33.4022 - rewards: -0.0672 - reward sum: -29418.1891 - t: 525.9900\n",
      "1000/1000 [==============================] - 526s 526ms/step - loss: 162.6213 - mean q: 32.3016 - rewards: -0.0428 - reward sum: -29461.0112 - t: 515.3832\n",
      "1000/1000 [==============================] - 522s 522ms/step - loss: 171.1647 - mean q: 30.8339 - rewards: -0.2216 - reward sum: -29682.6404 - t: 530.7173\n",
      "1000/1000 [==============================] - 527s 527ms/step - loss: 150.2308 - mean q: 32.2829 - rewards: -0.5662 - reward sum: -30248.8064 - t: 533.6047\n",
      "1000/1000 [==============================] - 525s 525ms/step - loss: 159.1188 - mean q: 32.5892 - rewards: 0.0297 - reward sum: -30219.0986 - t: 541.7503\n",
      " 837/1000 [========================>.....] - ETA: 1:27 - loss: 153.3067 - mean q: 31.8375 - rewards: 0.1499 - reward sum: -30093.6699 - t: 529.0933"
     ]
    }
   ],
   "source": [
    "x = environment()\n",
    "print(\"training...\")\n",
    "n = 100000000\n",
    "agent.train(num_steps = n, env = x, warmup = 0, log_interval = 1000, train_steps_per_step=1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a0a8f-de84-4a3b-a7ee-d53989c83bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.batch_size = 128\n",
    "#agent.gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0925b-cc30-4307-ae9f-7ed85cd3378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58a90cc1-b397-4a73-89ff-2899e08f3d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory = m\n",
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8df01-5584-4d3d-b34e-ebce946dcf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bc43f-23ef-487d-aea0-ae988e774cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
