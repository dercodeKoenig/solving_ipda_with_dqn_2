{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30e6f3-94f2-4fcf-9793-821d75d1f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd    \n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "name = \"dqn_trading_transformer_small\"\n",
    "resume = True\n",
    "#resume = False\n",
    "\n",
    "warmup_parallel = 4\n",
    "train_parallel = 4\n",
    "warmup_steps = 1000\n",
    "\n",
    "lr = 0.001\n",
    "memory_size = 32000\n",
    "gamma = 0.95\n",
    "exploration = 0.02\n",
    "target_model_sync = 5000\n",
    "batch_size = 32\n",
    "\n",
    "dlen = 120\n",
    "pos_size = 0.02 * 100000\n",
    "comm = 15/100000\n",
    "res_high = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1857a-99cd-4633-a6fb-0616a475f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "    print(\"created ./logs\")\n",
    "\n",
    "def Load(file):\n",
    "    f = open(file, \"rb\")\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72913f11-cf58-41b8-bcf5-34293e18b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class candle_class:\n",
    "    def __init__(self, o,h,l,c,t):\n",
    "        self.o=o\n",
    "        self.h=h\n",
    "        self.l=l\n",
    "        self.c=c\n",
    "        self.t=t\n",
    "\n",
    "class environment:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"../archive\"\n",
    "        #print(self.files)\n",
    "        #self.reset()\n",
    "\n",
    "    def reset(self, first = False):\n",
    "        self.files = [self.data_dir+\"/\"+x for x in os.listdir(self.data_dir) if \"candle_classes\" in x]\n",
    "        p = random.choice(self.files)\n",
    "        print(\"using\",p)\n",
    "        self.candles = Load(p)\n",
    "        #print(self.candles)\n",
    "        \n",
    "        \n",
    "        self.current_index = 0\n",
    "        if first:\n",
    "            self.current_index = random.randint(0,len(self.candles)-50000)\n",
    "            \n",
    "        self.d1_candles = deque(maxlen = dlen)\n",
    "        self.h4_candles = deque(maxlen = dlen)\n",
    "        self.h1_candles = deque(maxlen = dlen)\n",
    "        self.m15_candles = deque(maxlen = dlen)\n",
    "        \n",
    "        self.position = 0\n",
    "        self.entry_price = 0\n",
    "        self.equity = 0\n",
    "        self.current_equity = 0\n",
    "        self.balance = 0\n",
    "        \n",
    "            \n",
    "        self.get_sample_candles()\n",
    "        return [self.scale_candles(self.m15_candles), self.scale_candles(self.h1_candles), self.scale_candles(self.h4_candles), self.scale_candles(self.d1_candles), self.position]\n",
    "            \n",
    "    def close(self):\n",
    "        if self.position !=0:\n",
    "            self.balance = self.equity\n",
    "            self.position = 0\n",
    "        \n",
    "    def step(self, action) :\n",
    "        last_equity = self.equity\n",
    "        reset_entry_price = False\n",
    "        if action == 1: # long\n",
    "            if self.position != 1:\n",
    "                self.close()\n",
    "                self.position = 1\n",
    "                self.balance -= pos_size * comm\n",
    "                reset_entry_price = True\n",
    "                \n",
    "        if action == 0: # short\n",
    "            if self.position != -1:\n",
    "                self.close()\n",
    "                self.position = -1\n",
    "                self.balance -= pos_size * comm\n",
    "                reset_entry_price = True\n",
    "        \n",
    "        \n",
    "        if self.get_sample_candles() == -1:\n",
    "            print(\"error\")\n",
    "            return -1\n",
    "            \n",
    "        current_close = self.m15_candles[-1].c\n",
    "        if reset_entry_price: self.entry_price = self.m15_candles[-1].o\n",
    "        \n",
    "        percent_change = (current_close - self.entry_price) / self.entry_price\n",
    "\n",
    "        self.equity = self.balance + percent_change * pos_size * self.position\n",
    "        \n",
    "        reward = self.equity - last_equity\n",
    "        next_observation = [self.scale_candles(self.m15_candles), self.scale_candles(self.h1_candles), self.scale_candles(self.h4_candles), self.scale_candles(self.d1_candles), self.position]\n",
    "            \n",
    "        return next_observation, reward, len(self.candles) == self.current_index\n",
    "        \n",
    "        \n",
    "    def get_sample_candles(self):\n",
    "        if len(self.candles) == self.current_index:\n",
    "            return -1\n",
    "        while True:\n",
    "            # return dlen candles of d1, h4, h1 and m15\n",
    "            current_candle = self.candles[self.current_index]\n",
    "            current_hour = int(current_candle.t.split(\":\")[0])\n",
    "            current_closing_minute = int(current_candle.t.split(\":\")[1])\n",
    "\n",
    "            # m15 candles:\n",
    "            open_minute = int(current_closing_minute / 15) * 15 # candle saved the last minute but opening minute is better to use\n",
    "            self.m15_candles.append(candle_class(current_candle.o, current_candle.h, current_candle.l, current_candle.c, str(current_hour) +\":\"+str(open_minute)))\n",
    "\n",
    "            # h1 candles:\n",
    "            if  open_minute == 0: # a new hour candle started\n",
    "                new_candle = candle_class(current_candle.o, current_candle.h, current_candle.l, current_candle.c, str(current_hour)+\":00\")\n",
    "                self.h1_candles.append(new_candle)\n",
    "            else:\n",
    "                if len(self.h1_candles) > 0:\n",
    "                    self.h1_candles[-1].c = current_candle.c\n",
    "                    self.h1_candles[-1].h = max(current_candle.h, self.h1_candles[-1].h)\n",
    "                    self.h1_candles[-1].l = min(current_candle.l, self.h1_candles[-1].l)\n",
    "\n",
    "            # h4 candles:\n",
    "            # create a new h4 candle when hour is 17, 21, 1, 5, 9, 13\n",
    "            if  (current_hour == 17 or current_hour == 21 or current_hour == 1 or current_hour == 5 or current_hour == 9 or current_hour == 13) and open_minute == 0:\n",
    "                new_candle = candle_class(current_candle.o, current_candle.h, current_candle.l, current_candle.c, str(current_hour)+\":00\")\n",
    "                self.h4_candles.append(new_candle)\n",
    "            else:\n",
    "                if len(self.h4_candles) > 0:\n",
    "                    self.h4_candles[-1].c = current_candle.c\n",
    "                    self.h4_candles[-1].h = max(current_candle.h, self.h4_candles[-1].h)\n",
    "                    self.h4_candles[-1].l = min(current_candle.l, self.h4_candles[-1].l)\n",
    "\n",
    "            # d1 candles:\n",
    "            # create a new d1 candle when hour is 17\n",
    "            if  current_hour == 17 and open_minute == 0:\n",
    "                new_candle = candle_class(current_candle.o, current_candle.h, current_candle.l, current_candle.c, str(current_hour)+\":00\")\n",
    "                self.d1_candles.append(new_candle)\n",
    "            else:\n",
    "                if len(self.d1_candles) > 0:\n",
    "                    self.d1_candles[-1].c = current_candle.c\n",
    "                    self.d1_candles[-1].h = max(current_candle.h, self.d1_candles[-1].h)\n",
    "                    self.d1_candles[-1].l = min(current_candle.l, self.d1_candles[-1].l)\n",
    "\n",
    "            self.current_index+=1    \n",
    "            if len(self.d1_candles) == dlen:\n",
    "                break\n",
    "\n",
    "        return self.m15_candles,  self.h1_candles, self.h4_candles, self.d1_candles\n",
    "    \n",
    "    \n",
    "    def scale_candles(self, candles):\n",
    "        def scale_p(p):\n",
    "            return int((p - max_l) / hlrange * (res_high))\n",
    "        max_h = 0\n",
    "        max_l = 1000000\n",
    "        for i in candles:\n",
    "            if i.h > max_h:\n",
    "                max_h = i.h\n",
    "            if i.l < max_l:\n",
    "                max_l = i.l\n",
    "        hlrange = max_h - max_l\n",
    "        \n",
    "        \n",
    "        def scale_time(t):\n",
    "            hour = int(t.split(\":\")[0])\n",
    "            minute = int(t.split(\":\")[1])\n",
    "            total = hour * 60 + minute\n",
    "            max_t = 24*60\n",
    "            scaled = total / max_t\n",
    "            return scaled\n",
    "            \n",
    "        \n",
    "        \n",
    "        image = []\n",
    "        for i in candles:\n",
    "            clm = np.zeros(shape = (res_high+1))\n",
    "            color = 1 if i.o<i.c else -1\n",
    "            high_scaled = scale_p(i.h)\n",
    "            low_scaled = scale_p(i.l)\n",
    "            clm[low_scaled:high_scaled] = 0.5 * color\n",
    "            open_scaled = scale_p(i.o)\n",
    "            close_scaled = scale_p(i.c)\n",
    "            if color == 1:\n",
    "                clm[open_scaled:close_scaled+1] = color\n",
    "            if color == -1:\n",
    "                clm[close_scaled:open_scaled+1] = color\n",
    "                \n",
    "            c_time = scale_time(i.t)\n",
    "            clm[-1] = c_time\n",
    "            image.append(clm)\n",
    "        \n",
    "        current_close = candles[-1].c\n",
    "        scaled_close = scale_p(current_close)\n",
    "        clm = np.zeros(shape = (res_high+1))\n",
    "        clm[scaled_close] = 1\n",
    "        image.append(clm)\n",
    "        \n",
    "        return np.array(image).T\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45129f-c84c-4ca0-b6ea-94cbf198af0d",
   "metadata": {},
   "source": [
    "x = environment()\n",
    "m15,h1,h4,d1,pos = x.reset(True)\n",
    "plt.figure(figsize =(15,10))\n",
    "plt.imshow(m15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46640191-4de7-4379-b28a-b26c1567736c",
   "metadata": {},
   "source": [
    "#x = tf.keras.layers.Input(shape = (res_high+1, dlen+1))\n",
    "x = tf.convert_to_tensor(np.array(m15).reshape(1,res_high+1, dlen+1))\n",
    "#x1 = image\n",
    "#x2 = time\n",
    "x1 = x[::, :-1, :-1]\n",
    "x2 = x[::,-1,:-1]\n",
    "current_pos = x[::,::, -1]\n",
    "print(x.shape)\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(current_pos.shape)\n",
    "\n",
    "plt.figure(figsize =(15,10))\n",
    "plt.imshow(x1.numpy()[0])\n",
    "plt.show()\n",
    "plt.figure(figsize =(15,10))\n",
    "plt.imshow(x2.numpy())\n",
    "plt.show()\n",
    "plt.figure(figsize =(15,10))\n",
    "plt.imshow(current_pos.numpy())\n",
    "plt.show()\n",
    "plt.figure(figsize =(15,10))\n",
    "plt.imshow(x.numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d93b96-6791-4063-8051-069a9f830501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.05, **kwargs):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(TransformerBlock, self).get_config()\n",
    "        base_config['embed_dim'] = self.embed_dim\n",
    "        base_config['num_heads'] = self.num_heads\n",
    "        base_config['ff_dim'] = self.ff_dim\n",
    "        base_config['rate'] = self.rate\n",
    "        return base_config\n",
    "    \n",
    "    \n",
    "    \n",
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim, **kwargs):\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "        base_config['maxlen'] = self.maxlen\n",
    "        base_config['embed_dim'] = self.embed_dim\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701228a-b593-43e5-a4fa-ddf2063b9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def proc_chart(x):\n",
    "    #x1 = image\n",
    "    #x2 = time\n",
    "    x1 = x[::, :-1, :-1]\n",
    "    x2 = x[::,-1,:-1]\n",
    "    current_pos = x[::,::, -1]\n",
    "\n",
    "    x1 = tf.keras.layers.Reshape((res_high, dlen, 1))(x1)\n",
    "    \n",
    "    x5 = tf.keras.layers.Conv2D(16, 3,activation=\"relu\", padding=\"same\")(x1)\n",
    "    x1 = tf.keras.layers.Concatenate()([x1,x5])\n",
    "    x5 = tf.keras.layers.Conv2D(16, 3,activation=\"relu\", padding=\"same\")(x1)\n",
    "    x1 = tf.keras.layers.Concatenate()([x1,x5])\n",
    "    #x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    x1 = tf.keras.layers.Dense(12)(x1)\n",
    "    \n",
    "    x1 = tf.transpose(x1,perm=[0, 2, 1, 3])\n",
    "    x1 = tf.keras.layers.Reshape((dlen, res_high*x1.shape[-1]))(x1)\n",
    "    x2 = tf.keras.layers.Reshape((dlen, 1))(x2)\n",
    "    x1 = tf.keras.layers.Concatenate()([x1,x2])\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(256)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(256)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(96)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    \n",
    "    \n",
    "    x1 = PositionEmbedding(dlen, x1.shape[-1])(x1)\n",
    "    x1 = TransformerBlock(x1.shape[-1], 8, 256)(x1)\n",
    "    x1 = TransformerBlock(x1.shape[-1], 8, 256)(x1)\n",
    "    x1 = TransformerBlock(x1.shape[-1], 8, 256)(x1)\n",
    "    x1 = TransformerBlock(x1.shape[-1], 8, 256)(x1)\n",
    "\n",
    "    x1 = tf.keras.layers.Dense(256)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(32)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(16)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    \n",
    "    x1 = tf.keras.layers.Concatenate()([x1,current_pos])\n",
    "    x1 = tf.keras.layers.Dense(1024)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(256)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.LayerNormalization()(x1)\n",
    "    return x1\n",
    "    \n",
    "if True:\n",
    "    input_m15 = tf.keras.layers.Input(shape = (res_high+1, dlen+1))\n",
    "    input_h1 = tf.keras.layers.Input(shape = (res_high+1, dlen+1))\n",
    "    input_h4 = tf.keras.layers.Input(shape = (res_high+1, dlen+1))\n",
    "    input_d1 = tf.keras.layers.Input(shape = (res_high+1, dlen+1))\n",
    "    \n",
    "    x1 = proc_chart(input_m15)\n",
    "    x2 = proc_chart(input_h1)\n",
    "    x3 = proc_chart(input_h4)\n",
    "    x4 = proc_chart(input_d1)\n",
    "    \n",
    "    input_net_position = tf.keras.layers.Input(shape = (1))\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x1,x2,x3,x4,input_net_position])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dense(1024)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(2, activation = \"linear\", use_bias=False, dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model([input_m15,input_h1,input_h4, input_d1, input_net_position], outputs)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835645d-95a9-484d-b4d3-04ce14fb2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states = m15candles, h1candles, h4candles, d1candles, position\n",
    "#states =(5,dlen), (5,dlen), (5,dlen), (5,dlen), (1)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 100000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 1000,\n",
    "                 exploration = 0.01\n",
    "                ):\n",
    "        self.exploration = exploration\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_sync = target_model_sync\n",
    "   \n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "      \n",
    "    \n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name, overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = True)\n",
    "    def model_call(self, x):\n",
    "        x1, x2, x3, x4, x5 = x\n",
    "        return tf.math.argmax(self.model([x1,x2,x3,x4,x5]), axis = 1)\n",
    "    \n",
    "    def select_actions(self, state1, state2, state3, state4, state5):\n",
    "        if np.random.random() < self.exploration: # random action\n",
    "            return [np.random.randint(0,self.n_actions) for _ in range(len(state5))]\n",
    "        \n",
    "        ret = self.model_call([state1, state2, state3, state4, state5])\n",
    "        return ret.numpy()\n",
    "\n",
    "\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state])\n",
    "        \n",
    "    @tf.function(jit_compile = True)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.target_model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "\n",
    "        \n",
    "    @tf.function(jit_compile = True)\n",
    "    def tstep(self, data):\n",
    "        states, next_states, rewards, terminals, masks = data\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            model_return = self.model(states, training=True) \n",
    "            mask_return = model_return * masks\n",
    "            estimated_q_values = tf.math.reduce_sum(mask_return, axis=1)\n",
    "            #print(estimated_q_values, mask_return, model_return, masks)\n",
    "            loss_e = tf.math.square(target_q_values - estimated_q_values)\n",
    "            loss = tf.reduce_mean(loss_e)\n",
    "        \n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values)\n",
    "    \n",
    "    \n",
    "    def data_get_func(self):\n",
    "        idx = np.random.randint(0, len(self.memory), self.batch_size)\n",
    "        sarts_batch = [self.memory[i] for i in idx]\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_1 = np.array([x[0] for x in states], dtype=\"float32\")\n",
    "        states_2 = np.array([x[1] for x in states], dtype=\"float32\")\n",
    "        states_3 = np.array([x[2] for x in states], dtype=\"float32\")\n",
    "        states_4 = np.array([x[3] for x in states], dtype=\"float32\")\n",
    "        states_5 = np.array([x[4] for x in states], dtype=\"float32\")\n",
    "        \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_1 = np.array([x[0] for x in next_states], dtype=\"float32\")\n",
    "        next_states_2 = np.array([x[1] for x in next_states], dtype=\"float32\")\n",
    "        next_states_3 = np.array([x[2] for x in next_states], dtype=\"float32\")\n",
    "        next_states_4 = np.array([x[3] for x in next_states], dtype=\"float32\")\n",
    "        next_states_5 = np.array([x[4] for x in next_states], dtype=\"float32\")\n",
    "        #print(actions)\n",
    "        masks = np.array(self.m1[actions])\n",
    "        return [states_1, states_2, states_3, states_4, states_5], [next_states_1, next_states_2, next_states_3, next_states_4, next_states_5], rewards, terminals, masks\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.total_steps_trained+=1\n",
    "        if self.total_steps_trained % self.target_model_sync == 0:\n",
    "            self.copy_weights()\n",
    "\n",
    "           \n",
    "        data = self.data_get_func()\n",
    "        result= self.tstep(data)\n",
    "   \n",
    "        return  result\n",
    "    \n",
    "    def train(self, num_steps, envs, log_interval = 1000, warmup = 0, train_steps_per_step = 1):\n",
    "        self.total_steps_trained = -1\n",
    "\n",
    "        num_envs = len(envs)\n",
    "        states = [x.reset(True) for x in envs]\n",
    "        \n",
    "        current_episode_reward_sum = 0\n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.longs = 0\n",
    "        self.shorts = 0\n",
    "\n",
    "        self.total_rewards = []\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        \n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            file = open(log_folder+\"logs/loss_log.txt\", \"a\")  \n",
    "            #for loss in self.losses:\n",
    "                        #file.write(str(loss))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.losses)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/qv_log.txt\", \"a\")  \n",
    "            #for qv in self.q_v:\n",
    "                        #file.write(str(qv))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.q_v)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/rewards_log.txt\", \"a\")  \n",
    "            #for total_reward in self.total_rewards:\n",
    "                        #file.write(str(total_reward))\n",
    "                        #file.write(\"\\n\")\n",
    "                    \n",
    "            file.write(str(np.mean(self.total_rewards)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.total_rewards = []\n",
    "            self.losses = [0]\n",
    "            self.q_v = [0]\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.1, stateful_metrics = [\"reward sum\", \"t\", \"l/s\"])\n",
    "                    self.longs = 0\n",
    "                    self.shorts = 0\n",
    "\n",
    "\n",
    "                states_1 = np.array([x[0] for x in states])\n",
    "                states_2 = np.array([x[1] for x in states])\n",
    "                states_3 = np.array([x[2] for x in states])\n",
    "                states_4 = np.array([x[3] for x in states])\n",
    "                states_5 = np.array([x[4] for x in states])\n",
    "                \n",
    "                actions = self.select_actions(states_1, states_2, states_3, states_4, states_5)\n",
    "                for action in actions:\n",
    "                    if action == 0:\n",
    "                        self.shorts+=1\n",
    "                    elif action == 1:\n",
    "                        self.longs+=1\n",
    "\n",
    "                sasrt_pairs = []\n",
    "                for index in range(num_envs):\n",
    "                    sasrt_pairs.append([states[index], actions[index]]+[x for x in envs[index].step(actions[index])])\n",
    "\n",
    "                next_states = [x[2] for x in sasrt_pairs]\n",
    "\n",
    "                reward = [x[3] for x in sasrt_pairs]\n",
    "                current_episode_reward_sum += np.sum(reward)\n",
    "\n",
    "                self.total_rewards.extend(reward)\n",
    "\n",
    "                for index, o in enumerate(sasrt_pairs):\n",
    "                    #print(o)\n",
    "                    if o[4] == True:\n",
    "                        next_states[index] = envs[index].reset()\n",
    "                    self.observe_sasrt(o[0], o[1], o[2], o[3], o[4])\n",
    "\n",
    "                states = next_states\n",
    "                if i > warmup:\n",
    "                    for _ in range(train_steps_per_step):\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "                if (i+1) % log_interval == 0:\n",
    "                    #print(\"-----------\")\n",
    "                    #print(\"l:\", np.mean(self.losses))\n",
    "                    #print(\"q:\", np.mean(self.q_v))\n",
    "                    #print(\"reward sum\", current_episode_reward_sum)\n",
    "                    #print(\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts))\n",
    "                    #print(\"t\", np.mean(times))\n",
    "                    #print(\"-----------\")\n",
    "                    save_current_run()\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-train_steps_per_step:])),\n",
    "                                (\"mean q\", np.mean(self.q_v[-train_steps_per_step:])),\n",
    "                                (\"rewards\", np.mean(reward)),\n",
    "                                (\"reward sum\", current_episode_reward_sum),\n",
    "                                (\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts)),\n",
    "                                (\"t\", np.mean(times))])\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db0d3e-7df8-4a56-b06c-ea9aeb7a1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "\n",
    "log_folder = \"./\"\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model = model, \n",
    "    n_actions = 2, \n",
    "    memory_size = memory_size, \n",
    "    gamma=gamma,\n",
    "    optimizer = opt,\n",
    "    batch_size = batch_size, \n",
    "    target_model_sync = target_model_sync,\n",
    "    exploration = exploration,\n",
    "    name=log_folder+name+\".h5\")\n",
    "\n",
    "if resume:\n",
    "\tprint(\"loading weights...\")\n",
    "\tagent.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411125ec-439f-4de3-bcc3-4e0abf9d3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [environment() for _ in range(warmup_parallel)]\n",
    "print(\"warmup...\")\n",
    "n = warmup_steps\n",
    "agent.train(num_steps = n, envs = x, warmup = n, log_interval = n, train_steps_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb27070-100e-4c75-b51d-3d30d7132ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a81a7b-54d5-4796-b801-0e98ffee8890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 678s 678ms/step - loss: 0.8498 - mean q: 0.0628 - rewards: -0.0240 - reward sum: -183.0614 - l/s: -0.3599 - t: 714.6697\n",
      "1000/1000 [==============================] - 673s 673ms/step - loss: 0.8377 - mean q: 0.0637 - rewards: 0.0054 - reward sum: -161.6213 - l/s: 0.3089 - t: 776.5718\n",
      "1000/1000 [==============================] - 680s 680ms/step - loss: 1.0282 - mean q: 0.0760 - rewards: -0.0200 - reward sum: -241.7285 - l/s: -0.0560 - t: 682.5094\n",
      "1000/1000 [==============================] - 721s 721ms/step - loss: 0.8241 - mean q: 0.0533 - rewards: -0.0368 - reward sum: -389.0728 - l/s: -0.3284 - t: 810.4127\n",
      "1000/1000 [==============================] - 782s 782ms/step - loss: 0.9146 - mean q: 0.0471 - rewards: 0.0119 - reward sum: -341.3687 - l/s: -0.1570 - t: 770.8377\n",
      "1000/1000 [==============================] - 834s 834ms/step - loss: 0.8941 - mean q: 0.0452 - rewards: 0.0193 - reward sum: -264.2189 - l/s: 0.5894 - t: 725.8509\n",
      "1000/1000 [==============================] - 1109s 1s/step - loss: 0.9657 - mean q: 0.0602 - rewards: -0.0305 - reward sum: -386.2928 - l/s: 0.1190 - t: 883.3538\n",
      " 708/1000 [====================>.........] - ETA: 3:59 - loss: 1.1724 - mean q: 0.0583 - rewards: 0.0056 - reward sum: -370.4582 - l/s: -0.0021 - t: 744.4541   "
     ]
    }
   ],
   "source": [
    "x = [environment() for _ in range(train_parallel)]\n",
    "print(\"training...\")\n",
    "n = 1000000000\n",
    "agent.train(num_steps = n, envs = x, warmup = 0, log_interval = 1000, train_steps_per_step=1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99668af5-9022-416d-9a39-46886ff14463",
   "metadata": {},
   "outputs": [],
   "source": [
    "rew = [i[2] for i in agent.memory]\n",
    "sorted(rew)[0:10], sorted(rew)[-10:][::-1], \" - \", np.mean([abs(i) for i in rew])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a23302-fdb4-45f5-a315-6cc784c94461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ba604-8d37-4c7c-957c-74e333f91a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225cbed-9425-4cf1-af1b-9b23242d418f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357d921-1a96-4ba9-b6d5-5d149087d4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
