{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9200395f-000b-4dd9-aa2b-57aa38480858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "use_jit = False\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 100000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 1000,\n",
    "                 exploration = 0.01,\n",
    "                ):\n",
    "        self.exploration = exploration\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_sync = target_model_sync\n",
    "        self.num_model_inputs = len(self.model.inputs)\n",
    "        if not os.path.exists(\"logs\"):\n",
    "            os.mkdir(\"logs\")\n",
    "            print(\"created ./logs\")\n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "      \n",
    "    \n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name+\".h5\", overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = use_jit)\n",
    "    def model_call(self, x):\n",
    "        return tf.math.argmax(self.model(x), axis = 1)\n",
    "    \n",
    "    def select_actions(self, states):\n",
    "        if np.random.random() < self.exploration: # random action\n",
    "            return [np.random.randint(0,self.n_actions) for _ in range(len(states))]\n",
    "        \n",
    "        ret = self.model_call(states)\n",
    "        return ret.numpy()\n",
    "\n",
    "\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state])\n",
    "        \n",
    "    @tf.function(jit_compile = use_jit)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.target_model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "\n",
    "        \n",
    "    @tf.function(jit_compile = use_jit)\n",
    "    def tstep(self, data):\n",
    "        states, next_states, rewards, terminals, masks = data\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            model_return = self.model(states, training=True) \n",
    "            mask_return = model_return * masks\n",
    "            estimated_q_values = tf.math.reduce_sum(mask_return, axis=1)\n",
    "            #print(estimated_q_values, mask_return, model_return, masks)\n",
    "            loss_e = tf.math.square(target_q_values - estimated_q_values)\n",
    "            loss = tf.reduce_mean(loss_e)\n",
    "        \n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values)\n",
    "    \n",
    "    \n",
    "    def data_get_func(self):\n",
    "        idx = np.random.randint(0, len(self.memory), self.batch_size)\n",
    "        sarts_batch = [self.memory[i] for i in idx]\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_array = []\n",
    "        if self.num_model_inputs == 1:\n",
    "            states_array = states\n",
    "        else:\n",
    "            for i in range(self.num_model_inputs):\n",
    "                states_array.append(np.array([x[i] for x in states]))\n",
    "        states_array = np.array(states_array)    \n",
    "                    \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_array = []\n",
    "        if self.num_model_inputs == 1:\n",
    "            next_states_array = next_states\n",
    "        else:\n",
    "            for i in range(self.num_model_inputs):\n",
    "                next_states_array.append(np.array([x[i] for x in next_states]))\n",
    "                \n",
    "        next_states_array = np.array(next_states_array)    \n",
    "        #print(actions)\n",
    "        masks = np.array(self.m1[actions])\n",
    "        return states_array, next_states_array, rewards, terminals, masks\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.total_steps_trained+=1\n",
    "        if self.total_steps_trained % self.target_model_sync == 0:\n",
    "            self.copy_weights()\n",
    "\n",
    "           \n",
    "        data = self.data_get_func()\n",
    "        result= self.tstep(data)\n",
    "   \n",
    "        return  result\n",
    "    \n",
    "    def train(self, num_steps, envs, log_interval = 1000, warmup = 0, use_reward_per_episode = False, render = False):\n",
    "        self.total_steps_trained = -1\n",
    "\n",
    "        num_envs = len(envs)\n",
    "        states = [x.reset() for x in envs]\n",
    "        \n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.rewards = [0]\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        if use_reward_per_episode:\n",
    "            env_reward_counter = [0 for _ in range(len(envs))]\n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            if len(self.losses) > 0:\n",
    "                file = open(\"logs/loss_log.txt\", \"a\")  \n",
    "                file.write(str(np.mean(self.losses)))\n",
    "                file.write(\"\\n\")\n",
    "                file.close()\n",
    "            if len(self.q_v) > 0:\n",
    "                file = open(\"logs/qv_log.txt\", \"a\")  \n",
    "                file.write(str(np.mean(self.q_v)))\n",
    "                file.write(\"\\n\")\n",
    "                file.close()\n",
    "\n",
    "            file = open(\"logs/rewards_log.txt\", \"a\")  \n",
    "            file.write(str(np.mean(self.rewards)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.rewards = []\n",
    "            self.losses = []\n",
    "            self.q_v = []\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.1, stateful_metrics = [\"t\", \"rewards\"])\n",
    "\n",
    "                states_array = []\n",
    "                if self.num_model_inputs == 1:\n",
    "                    states_array = states\n",
    "                else:\n",
    "                    for i in range(self.num_model_inputs):\n",
    "                        states_array.append(np.array([x[i] for x in states]))\n",
    "                \n",
    "                states_array = np.array(states_array)    \n",
    "                \n",
    "                actions = self.select_actions(states_array)\n",
    "                \n",
    "\n",
    "                sasrt_pairs = []\n",
    "                for index in range(num_envs):\n",
    "                    sasrt_pairs.append([states[index], actions[index]]+[x for x in envs[index].step(actions[index])])\n",
    "\n",
    "                next_states = [x[2] for x in sasrt_pairs]\n",
    "\n",
    "                reward = [x[3] for x in sasrt_pairs]\n",
    "                \n",
    "                if not use_reward_per_episode:\n",
    "                    self.rewards.extend(reward)\n",
    "                else:\n",
    "                    for o in range(len(env_reward_counter)):\n",
    "                        env_reward_counter[o] += reward[o]\n",
    "                    \n",
    "                for index, o in enumerate(sasrt_pairs):\n",
    "                    #print(o)\n",
    "                    if o[4] == True:\n",
    "                        next_states[index] = envs[index].reset()\n",
    "                        if use_reward_per_episode:\n",
    "                            self.rewards.append(env_reward_counter[index])\n",
    "                            env_reward_counter[index] = 0\n",
    "                    self.observe_sasrt(o[0], o[1], o[2], o[3], o[4])\n",
    "\n",
    "                states = next_states\n",
    "                if render:\n",
    "                    [x.render() for x in envs]\n",
    "                    \n",
    "                if i > warmup:\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-1]) if len(self.losses)>0 else 0),\n",
    "                                (\"mean q\", np.mean(self.q_v[-1]) if len(self.q_v)>0 else 0),\n",
    "                                (\"rewards\", np.mean(self.rewards) if len(self.rewards)>0 else 0),\n",
    "                                (\"t\", np.mean(times))], \n",
    "                              finalize = (i+1) % log_interval == 0)\n",
    "        \n",
    "                if (i+1) % log_interval == 0:\n",
    "                    save_current_run()\n",
    "                    \n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f38cff-3dae-4b3b-acfe-0916f103cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 531,972\n",
      "Trainable params: 531,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inp = tf.keras.layers.Input(shape = (8))\n",
    "x = tf.keras.layers.Dense(512, activation = \"relu\")(inp)\n",
    "x = tf.keras.layers.Dense(512, activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dense(512, activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dense(4, activation = \"linear\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = inp, outputs = x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1479996-5078-405a-ab09-2b0d919d4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created ./logs\n"
     ]
    }
   ],
   "source": [
    "x = DQNAgent(model = model,\n",
    "                 n_actions = 4,\n",
    "                 memory_size = 100000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.001), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =64,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 200,\n",
    "                 exploration = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1075b0e-0c2c-46b9-893a-804ad381cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "envs = [gym.make(\"LunarLander-v2\") for _ in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a4ed67-c40d-413a-bce6-be62fdf67004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 3ms/step - loss: 0.0000e+00 - mean q: 0.0000e+00 - rewards: -7.7529 - t: 3.3427\n",
      "1000/1000 [==============================] - 13s 14ms/step - loss: 67.9008 - mean q: -14.0850 - rewards: -1.5973 - t: 21.2633\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 36.2314 - mean q: -16.3181 - rewards: -0.5893 - t: 19.6095\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 34.5167 - mean q: -12.9785 - rewards: -0.2268 - t: 28.1796\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: 25.9482 - mean q: -10.3357 - rewards: -0.0943 - t: 29.3573\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 29.4974 - mean q: -7.2072 - rewards: -0.0902 - t: 28.3214\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 27.1264 - mean q: -4.3883 - rewards: -0.1198 - t: 24.7948\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 33.1672 - mean q: -0.2063 - rewards: -0.0076 - t: 20.76870s - loss: 33.1579 - mean q: -0.2781 - rewards: -0.0\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 31.0734 - mean q: 3.7688 - rewards: 0.1431 - t: 27.1735\n",
      "1000/1000 [==============================] - 25s 25ms/step - loss: 24.7654 - mean q: 9.8254 - rewards: 0.2514 - t: 26.46511s - loss: 24.9250 - mean q: 9.661\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 21.8098 - mean q: 14.1295 - rewards: -0.0537 - t: 26.7361\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 22.1919 - mean q: 16.2786 - rewards: 0.0345 - t: 24.6571\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 19.7192 - mean q: 19.0090 - rewards: 0.0517 - t: 25.7874\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 17.0236 - mean q: 26.1062 - rewards: 0.1923 - t: 27.5300\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 13.5718 - mean q: 28.1764 - rewards: 0.0404 - t: 26.2215\n",
      " 223/1000 [=====>........................] - ETA: 17s - loss: 15.3141 - mean q: 28.3555 - rewards: 0.2147 - t: 18.7039\n",
      "\n",
      "break!\n"
     ]
    }
   ],
   "source": [
    "x.train(30000, envs = envs, warmup = 1000, render = False, use_reward_per_episode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07cbc21-a1aa-4c1b-8945-8aa49f298719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 499/1000 [=============>................] - ETA: 12s - loss: 11.2522 - mean q: 28.7027 - rewards: 0.2126 - t: 26.5711"
     ]
    }
   ],
   "source": [
    "envs = [gym.make(\"LunarLander-v2\") for _ in range(2)]\n",
    "x.train(500, envs = envs, warmup = 0, render = True, use_reward_per_episode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1020d5c-f2d0-4bbf-a4b1-4e39e9c01253",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(envs)):\n",
    "    envs[i].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e170a8-bc36-4974-8883-7cebd5852ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
